<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Diffusion Language Models -- Part One (Introduction)</title>
  <meta name="description" content="Language modeling with diffusion architectures is gaining traction and there are several promising indicators for further adoption. Since I have been on a de...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
<!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  


  <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/print.css" media="print"> -->

  <link rel="canonical" href="http://localhost:4000/hankelvin.github.io/articles/25/Diffusion_LM_P1">

  <link rel="alternate" type="application/rss+xml" title="Kelvin's blog" href="http://localhost:4000/hankelvin.github.io/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<a href="/hankelvin.github.io/"><img class="badge" src="/hankelvin.github.io/assets/img/L1016424.png" alt="CH"></a>
	
		
  	
		
		    
		      <a href="/hankelvin.github.io/">Posts</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/page/">Stuff</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/css/print.css"></a>
		    
	    
  	
		
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/latex.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/main.css"></a>
		    
	    
  	
	</nav>
</header>
    <article class="group">
      <h1 style="max-width: 50%; line-height: 1.5">Diffusion Language Models -- Part One (Introduction)</h1>
<p class="subtitle">July 20, 2025</p>

<p><span class="newthought">Language modeling with diffusion architectures is gaining traction and there are several promising indicators for further adoption.</span> Since I have been on a deep dive into diffusion modeling<label for="sidenote-appreciation" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-appreciation" class="margin-toggle" checked="" /><span class="sidenote"><em>Impetus: the spark to look into diffusion language models came to mind during a presentation on multimodal AI at Lorong AI <a href="https://lorong.ai/">https://lorong.ai/</a> (they do a very nicely curated set of expert talks across wide swathes of AI and AI-adjacent topics). Whilst sitting in one on how vision-language models (<strong>VLMs</strong>) may be relevant to actions planning in robotics, and how latency is crucial for such uses, I started to wonder about non-autoregressive approaches, especially diffusion (which is known for better consistency and potential for speed). It also helped that it was Google‚Äôs I/O week and amidst the coverage of their various launches were mentions of a diffusion demo (see below).</em></span> ‚Äì including some experiments for reinforcement learning (<strong>RL</strong>) post-training of a diffusion language model (<strong>DLM</strong>), I thought to share what I have come across in a series of posts; which I am planning as (at least) a four-parter, to be gradually released over the next few weeks.<label for="sidenote-weekly" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-weekly" class="margin-toggle" checked="" /><span class="sidenote">The plan is for updates each week. There are already well-written posts on DLMs: one by <a href="https://spacehunterinf.github.io/blog/2025/diffusion-language-models/">Xiaochen Zhu</a> (Apr 2025) and one by <a href="https://sander.ai/2023/01/09/diffusion-language.html">Sander Dielman</a> (2023), so I will not reinvent the wheel and will focus mostly on outlining and explaining DLM developments since then/as yet covered.</span> My focus will be on diffusion for text<label for="sidenote-MDLM" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-MDLM" class="margin-toggle" checked="" /><span class="sidenote">Especially masked diffusion language models (<strong>MDLM</strong>), a variant whereby a special [MASK] token is used (also termed an <em>absorbing state</em>; in the forward process, MDLMs noise the original sequences by transitioning tokens to this state ‚Äì i.e. [MASK]). Training and sampling has been found to be easier when modeling the diffusion process for discrete sequences in this way.</span>; which is the modality I am most familiar with; although these models can be applied to discrete sequences in general as well as to, or together with, other modalities (e.g. vision ‚Äì which diffusion models were originally developed on, as well as audio). In this first post I will introduce DLMs briefly and outline why I see them as promising.</p>

<h3 id="-1-what-are-dlms-how-are-they-different-from-current-llms">üìù 1. What are DLMs? How are they different from current LLMs?</h3>

<p><span class="newthought">Diffusion architectures are designed around a training and inference procedure that involves a forward as well as a reverse process.</span> In the forward process, random noise is added to some original input (e.g. a real image or a human-written text) and repeatedly done so until it is entirely noised (i.e. the structure that was in the input is completely lost<label for="sidenote-noise" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-noise" class="margin-toggle" checked="" /><span class="sidenote">In the form of white noise for images; gibberish (or complete whitespace in the MDLM case) for text.</span>). What we want is a model that can seek to reverse this noising process; and if the noise in the forward process has been carefully added (minute amounts at each step; following an increasing schedule; and from a distribution easy to sample from), then the learning of the model is made relatively easy. Subsequently, it will be possible to start from complete noise as input, denoise it over some number of steps using the learned model, and arrive at a state where meaningful structure is restored (<em>et voila</em>, we would have obtained a realistic sample).</p>

<p><label for="marginfigure-wof" class="margin-toggle">‚äï</label><input type="checkbox" id="marginfigure-wof" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/wheeloffortune.png" /><br /><em>Generated with ChatGPT.</em></span></p>

<p><span class="newthought">To give a more intuitive sense of how DLMs work</span>, I will draw on Wheel of Fortune (<strong>Wheel</strong>)<label for="sidenote-genz" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-genz" class="margin-toggle" checked="" /><span class="sidenote"><em>Sidetrack: For an introduction, see this <a href="https://en.wikipedia.org/wiki/Wheel_of_Fortune_(American_game_show)">Wikipedia</a> entry.</em></span> for an analogy. <ins>To set the scene</ins>: <em>imagine it is a weekday evening, a game with Pat Sajak (or Ryan Seacrest if you prefer) and Vanna White is running, a sole contestant remains on the show. The round starts and the board shuffles to reveal a sequence of white panels, and Pat/Ryan gives the category: ‚ÄúLiving Things‚Äù.</em> 
So far this corresponds to the forwards noising process described above.</p>

<p>In this game you are Pat/Ryan (who gave the category), and the contestant is your favourite LLM (ChatGPT, Claude, Deepseek, Le Chat etc) or a DLM. The task is for the contestant to correctly guess the characters behind each of these white panels on the board, based on the category.<label for="sidenote-rules" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-rules" class="margin-toggle" checked="" /><span class="sidenote"><em>Sidetrack: In fact we can drop the wheel; there is no revealing of R-S-T-L-N-E on the board either; and guesses are for tokens not characters. Think also of the category along the lines of the context/prompt we typically give to LLMs and the white panels on the Wheel board as the LLM/DLM‚Äôs response to your context/prompt.</em></span> The LLMs (e.g. ChatGPT) that we are familiar with are modeled in an auto-regressive manner (<strong>AR-LLMs</strong> from hereon), i.e. no matter what, they go about solving the task by making a sequence of guesses that go from left to right, one character/token at a time.<label for="sidenote-unlikely" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-unlikely" class="margin-toggle" checked="" /><span class="sidenote">If you think about it: such a strictly left-to-right strategy is unlikely to be adopted by a human player.</span> A DLM, on the other hand, solves the task by making a sequence of guesses (the reverse process above) ‚Äì each guess can be for anywhere across the sequence and can also be for multiple tokens at a time. At each step of the guessing, a DLM makes its next guess based on what it has already unmasked in the sequence.</p>

<p>Although this is a major simplication ‚Äì for now, I have glossed over many details and important nuances for DLMs ‚Äì it should have hopefully given you a rather concrete sense of how AR-LLMs/DLMs generate text conditionally. As you can see, the AR-LLM and DLM generate text in ways that are quite different.</p>

<p><label for="marginnote-caveats" class="margin-toggle"> ‚äï</label><input type="checkbox" id="marginnote-caveats" class="margin-toggle" checked="" /><span class="marginnote"><em>Some other caveats for the Wheel analogy: it is not perfect for AR-LLMs, because they only know to keep guessing characters/tokens until some special stop state has been reached (i.e. without knowledge of how many characters/tokens left to guess; the white panels). It is also not a perfect analogy for DLMs because in practice (and also a source for their appeal) they predict more than one character/token at a time.</em></span></p>

<h3 id="-2-what-is-the-appeal-of-dlms">ü§© 2. What is the appeal of DLMs?</h3>

<p>A major appeal of DLMs is its <strong>non-autoregressive manner (NAR)</strong> of generation<label for="sidenote-autoreg" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-autoreg" class="margin-toggle" checked="" /><span class="sidenote">Technically, we can make a DLM autoregressive ‚Äì we can enforce a left-to-right denoising during inference; and we can modify the DLM training to go to a noising schedule that runs right-to-left.</span>. For text, this reflects more closely how long-form writing (typically requiring planning) takes place. In constrast, the auto-regressive (left-to-right) manner of generation places a very strong constraint, and there has long been grouses about it, as well as claims that it is unlikely to bring us to stronger machine intelligence<label for="sidenote-errorgen" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-errorgen" class="margin-toggle" checked="" /><span class="sidenote">Yann LeCun then-controversially stated at a few venues in 2023 (see below a slide he reposted on X) that AR-LLMs, in having to generate tokens one-by-one, encounter errors that compound. If the generation path veers into a region away from the right answer, there is no (straightfoward) way for the AR-LLM to get to the right answer. It is worth noting however, that methods to induce test-time-scaling (with post-training such as iterative supervised fine-tuning or RL for reasoning) mitigate at least a part of this issue, without having to switch to NAR; (look under <em>`Why Yann Lecun was wrong (kind of)‚Äô</em> in this <a href="https://blog.jxmo.io/p/we-should-stop-talking-about-agi">post</a> by Jack Morris).</span>.</p>

<p><label for="marginfigure-yann" class="margin-toggle">‚äï</label><input type="checkbox" id="marginfigure-yann" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/yannlecun_autoregressive.png" /><br />Source: Slide ‚Äì Yann LeCun‚Äôs <a href="https://x.com/ylecun/status/1640122342570336267">X</a> post in 2023</span></p>

<p>From this non- <em>autoregressivity</em> springs other benefits. Since DLMs are not limited to generating one token at a time, (i) there is potential for <strong>significant speed-up</strong> with it<label for="sidenote-dlmspeed" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-dlmspeed" class="margin-toggle" checked="" /><span class="sidenote">I am using ‚Äòpotential‚Äô here because efficient serving methods for AR-LLM such as KV caching and speculative decoding do not transfer directly to DLMs. There is a trade-off currently ‚Äì even if a DLM takes fewer steps than AR-LLMs for inference, each of the DLM‚Äôs steps requires computing over the full target sequence length. It is not yet clear if similar efficient serving methods are available and can work well for DLMs. Furthermore, (at least for the current generation of DLMs from academia) running more denoising steps up to max sequence length is necessary for DLMs‚Äô peak performance (see Figure 5 of Appendix B6 of the LLaDA <a href="https://arxiv.org/pdf/2502.09992">paper</a>).</span>; and (ii) they can achieve better conditional <strong>control and consistency</strong><label for="sidenote-dlmcontrol" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-dlmcontrol" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://arxiv.org/abs/2205.14217">Diffusion-LM Improves Controllable Text Generation</a> (Li et al, 2022)</span>. Furthermore, generating in an NAR and denoising manner also enables alternative decoding strategies such as <strong>infilling</strong>. In the GIF below, some prompt is shown at the start for the DLM to complete, followed by a constraint that must be met at the end; the task is to generate some sequence of text between them (hence the term `infilling‚Äô<label for="sidenote-infilling" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-infilling" class="margin-toggle" checked="" /><span class="sidenote">See also <a href="https://stable-diffusion-art.com/inpainting_basics/">inpainting</a> for vision models.</span>), which is not easily possible with an AR-LLM (without restarting inference at the point where the edit(s) end). Infilling is useful in code generation ‚Äì for instance, a program could be generated and an engineer can make targeted modifications (say a function) anywhere within the program, and then generation can continue based on this modified state. I find this infilling capability of DLMs particularly attractive; its usefulness is not restricted to code generation, and having it for generative modeling allows for application design choices that can facilitate mutually beneficial human-machine collaboration; if done right, these infilled edits are useful for improving model outputs (towards general model capabilities and for meeting localised/personalised preferences).</p>

<figure><figcaption><span>Source: Diffusion generation GIF ‚Äì HKUNLP‚Äôs blog <a href="https://hkunlp.github.io/blog/2025/dream/">post</a> on Dream. <em>Uber-sidetrack: see also start of the end credits in Claire Denis‚Äô <a href="https://youtu.be/grGiq0yTaj4?t=214">Beau Travail</a></em></span></figcaption><img src="/hankelvin.github.io/assets/img/fig_infill_1.gif" /></figure>

<!-- self-correction  -->
<!-- There are however still remaining issues that have not been resolved. Fixed token counts at the start. It is not yet clear if techniques with similar compute-saving effects to KV caching can be applied -->

<h3 id="-3-what-are-the-signals-for-dlms-potential">üö¶ 3. What are the signals for DLMs‚Äô potential?</h3>

<p><span class="newthought">Firstly, two commerical-grade models are already available.</span> They provide strong support for the potential of DLMs; I have tried demos of both, and they are very compelling for general purpose LLM usage.</p>

<p>‚óºÔ∏è The first to arrive was Mercury. In fact, the Mercury models are already products accessible via API<label for="sidenote-mercuryapi" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-mercuryapi" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://platform.inceptionlabs.ai/docs#models">https://platform.inceptionlabs.ai/docs#models</a></span>. They are from Inception Labs, a start-up whose co-founders are Stefano Ermon (Stanford) and his former students Aditya Grover (UCLA) and Volodymyr Kuleshov (Cornell)<label for="sidenote-mercury-founders" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-mercury-founders" class="margin-toggle" checked="" /><span class="sidenote">Ermon - <a href="https://x.com/StefanoErmon">X</a>/<a href="https://cs.stanford.edu/~ermon">website</a>; Grover - <a href="https://x.com/adityagrover_">X</a>/<a href="https://aditya-grover.github.io/">website</a>; Kuleshov - <a href="https://x.com/volokuleshov">X</a>/<a href="https://www.cs.cornell.edu/~kuleshov/">website</a>.</span>. Notably, the founders are behind the research for many of the diffusion modeling innovations (for both image and discrete sequences) in recent years. Their code-focused DLM (Mercury Coder Mini/Small), that was released in February 2025<label for="sidenote-tcmercury" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-tcmercury" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://techcrunch.com/2025/02/26/inception-emerges-from-stealth-with-a-new-type-of-ai-model/">TechCrunch article on Inception Labs</a></span>, has been independently benchmarked as being able to generate at &gt;1,000 tokens per second; up to 10x faster than heavily optimised closed LLMs like GPT-4o Mini and Claude 3.5 Haiku (see chart below). Their`Mini‚Äô code model is also currently (July 2025) joint first on Copilot Arena<label for="sidenote-copilot" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-copilot" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://lmarena.ai/leaderboard/copilot">https://lmarena.ai/leaderboard/copilot</a></span>. In June 2025, they also released a general chat (like ChatGPT and Claude) model<label for="sidenote-mercury-chat" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-mercury-chat" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://www.inceptionlabs.ai/introducing-mercury-our-general-chat-model">Inception Labs release</a></span> .</p>

<!-- It was Ermon who was one of the first, if not the first, to propose the iterative denoising for generative modeling (for images); see [Generative Modeling by Estimating Gradients of the Data Distribution](https://arxiv.org/abs/1907.05600) NOTE: not true, most of the citations for diffusion models are Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models -->

<figure><figcaption><span>Chart from the Inception Labs <a href="https://www.inceptionlabs.ai/introducing-mercury">website</a></span></figcaption><img src="/hankelvin.github.io/assets/img/mercury_artificialanalysis.png" /></figure>

<p><strong>Tip</strong>: I recommend checking out Yupp.ai<label for="sidenote-yupp" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-yupp" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://yupp.ai/">https://yupp.ai</a>; <em>They have a very unique and practical value proposition</em> ‚Äì see this <a href="https://www.wired.com/story/yupp-chatbot-pays-users-ai-model-feedback/">Wired</a> profile of Yupp when they came out of stealth in June 2025. See the research thinking (including on global-scale localisable evaluations) behind Yupp <a href="https://x.com/lintool/status/1943013316428734763">here</a> as well as on their <a href="https://blog.yupp.ai/leaderboard">blog</a>. </span>, which you can use to easily test the Mercury models<label for="sidenote-mercury" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-mercury" class="margin-toggle" checked="" /><span class="sidenote">Find the <em>Select a model</em> button and search for and add <em>Inception Mercury</em> (the general chat model).</span> (or any other model) side-by-side against more than 600 other (open as well as closed-source) LLMs, including the latest and previous model versions from OpenAI, Anthropic etc.</p>

<p>‚óºÔ∏è The second is from Google Deepmind<label for="sidenote-google" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-google" class="margin-toggle" checked="" /><span class="sidenote">This demo is less accessible; you will have to sign up for it via a <a href="https://deepmind.google/models/gemini-diffusion/">waitlist</a>.</span> which landed in May 2025. They claim that their DLM generates up to ~1,500 tokens per second and achieves results nearly matching or even outperforming Gemini 2.0 Flash-Lite<label for="sidenote-geminiflash" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-geminiflash" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://deepmind.google/models/gemini/flash-lite/">Gemini 2.5 Flash Lite</a> is the smallest of the Gemini models (i.e. first come Pro, Flash and then Flash Lite).</span> for five out of six coding benchmarks evaluated against<label for="sidenote-codebenchmark" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-codebenchmark" class="margin-toggle" checked="" /><span class="sidenote">Except for SWE-Bench Verified, where the diffusion model obtained 22.9% vs the 28.5% obtained by Gemini 2.5 Flash Lite</span>, as well as stronger math performance (AIME 2025).</p>

<p>~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~</p>

<p><span class="newthought">Secondly, open-source/weights DLMs</span> have been trained and released by academia. This signals accessbility to training such models, at compute-levels available there. These models are performing favourably against comparable AR-LLMs on evaluations over a wide range of real-use cases<label for="sidenote-lmharness" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-lmharness" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a></span>, and not just on measurements of perplexity alone. Such models include the LLaDA family<label for="sidenote-llada" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-llada" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://github.com/ML-GSAI/LLaDA">https://github.com/ML-GSAI/LLaDA</a>; from Renmin University of China, BDAI and Ant Group</span>, which underwent pretraining and supervised fine-tuning at scales similar to recent AR-LLMs like Llama 3 (i.e. pretrained with Internet data at trillion token-scale ‚Äì for denoising instead of next-token prediction as in AR-LLMs, as well as supervised fine-tuned on millions of instruction-answer pairs for instruction-following). Notably, parallel efforts to use AR-LLMs‚Äô weights to initialise models for DLM training<label for="sidenote-arconversion" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-arconversion" class="margin-toggle" checked="" /><span class="sidenote">See <a href="https://arxiv.org/abs/2410.17891">DiffuLlama</a> and <a href="https://hkunlp.github.io/blog/2025/dream">Dream</a>.</span> are also showing promising performance ‚Äì these allow us to bypass the need to train DLMs from scratch, and would be a way to further leverage all the training that have already been expended on existing AR-LLMs.</p>

<p>~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~</p>

<p><span class="newthought">Thirdly, solutions are also emerging for preference tuning and RL post-training of DLMs.</span> The former have been important for enhancing safety and instruction-following of AR-LLMs, and the latter is increasingly important for unlocking their reasoning capabilities. Work done in this space include: (i) DiffuCoder<label for="sidenote-diffucoder" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-diffucoder" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://github.com/apple/ml-diffucoder">https://github.com/apple/ml-diffucoder</a>; from the University of Hong Kong and Apple.</span>; (ii) diffu-GRPO<label for="sidenote-diffgrpo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-diffgrpo" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://dllm-reasoning.github.io">https://dllm-reasoning.github.io</a>; from UCLA and Meta AI.</span>; and (iii) Variance-Reduced Preference Optimization (<strong>VRPO</strong>) in LLaDA 1.5<label for="sidenote-llada15" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-llada15" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://ml-gsai.github.io/LLaDA-1.5-Demo">https://ml-gsai.github.io/LLaDA-1.5-Demo</a>; from Renmin University of China, Tsinghua and Ant Group.</span>. Given the recent developments in these spaces for AR-LLMs, these are likely to be interesting areas to explore with DLMs.</p>

<!-- ~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~ -->

<!-- <span class='newthought'>Fourthly, applications to robotics</span> -- in specialised vision-language-action (VLA) models -- have been explored; and multi-modal vision-language DLMs, i.e. more general purpose capabilities, are also appearing.Discuss MaskGIT, mention LLaDA-V -->

<h3 id="-4-whats-next">üëâ 4. What‚Äôs next?</h3>
<p>In summary, I have briefly introduced diffusion language models (<strong>DLMs</strong>) which is an emerging approach for an alternative to current auto-regressive LLMs. I also discussed why DLMs are appealing and highlighted some indicators I believe show their promise. In my next post, I will go into how DLMs are trained/converted from AR-LLMs<label for="sidenote-dream" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-dream" class="margin-toggle" checked="" /><span class="sidenote">As in the case of models like <a href="https://hkunlp.github.io/blog/2025/dream/">Dream</a>.</span>, going a bit deeper into the technicals. Following that I will examine the ways to sampling from these models, with a discussion on possible implications on efficient serving.</p>



    </article>
    <span class="print-footer">Diffusion Language Models -- Part One (Introduction) - July 20, 2025 - Kelvin Han</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <!-- <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>     -->
    
      <li>
        <a href="//www.linkedin.com/in/kelvin-han"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="//www.x.com/kelvinhan"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//scholar.google.com/citations?user=AuOeIe8AAAAJ&hl=en"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//www.github.com/hankelvin"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2025 &nbsp;&nbsp;KELVIN HAN</span></br> <br>
<span>This site was created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte-Jekyll</a> theme in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
