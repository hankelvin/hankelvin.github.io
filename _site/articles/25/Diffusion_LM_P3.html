<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Diffusion Language Models -- Part Three (Generating with DLMs; through some art & linguistics)</title>
  <meta name="description" content="Generating text with DLMs is quite different from doing so with AR-LLMs, and in my earlier posts here and here I have sketched a brief outline of how it work...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
<!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  


  <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/print.css" media="print"> -->

  <link rel="canonical" href="http://localhost:4000/hankelvin.github.io/articles/25/Diffusion_LM_P3">

  <link rel="alternate" type="application/rss+xml" title="Kelvin's blog" href="http://localhost:4000/hankelvin.github.io/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<a href="/hankelvin.github.io/"><img class="badge" src="/hankelvin.github.io/assets/img/L1016424.png" alt="CH"></a>
	
		
  	
		
		    
		      <a href="/hankelvin.github.io/">Posts</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/page/">Stuff</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/css/print.css"></a>
		    
	    
  	
		
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/latex.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/main.css"></a>
		    
	    
  	
	</nav>
</header>
    <article class="group">
      <h1 style="max-width: 50%; line-height: 1.5">Diffusion Language Models -- Part Three (Generating with DLMs; through some art & linguistics)</h1>
<p class="subtitle">September 1, 2025</p>

<p><span class="newthought">Generating text with DLMs is quite different from doing so with AR-LLMs</span>, and in my earlier posts <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P1">here</a> and <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P2">here</a> I have sketched a brief outline of how it works for Masked-DLMs (using a Wheel of Fortune analogy). In this post, I will go a little deeper into the generation process and examine a number of limitations/challenges there, together with what has been recently proposed for addressing them. This post also starts with a slightly different flavour, a light detour from the so-far technical posts with a foray covering interesting artwork I saw recently, with associations to DLMs and which brought out connections to ideas in linguistics; I was thinking that starting this way could help ground the technical aspects of DLMs to some visual concepts which might aid in understanding these technical aspects. <label for="marginnote-delay" class="margin-toggle"> ‚äï</label><input type="checkbox" id="marginnote-delay" class="margin-toggle" checked="" /><span class="marginnote"><em>This post is a little delayed as I was engaged in some community duties and it also took some time to take a deeper investigation into some of the works covered in this post.</em></span></p>

<h3 id="Ô∏è-1-seeing-dlms-through-art-and-linguistics">üñºÔ∏è 1. Seeing DLMs through art and linguistics</h3>
<p>In July, I visited the <a href="https://www.instagram.com/hemanchong/" title="Heman Chong's Instagram">Heman Chong</a> retrospective at the <a href="https://www.singaporeartmuseum.sg/">Singapore Art Museum</a>, and encountered a piece of work that made me smile<label for="marginnote-heman" class="margin-toggle"> ‚äï</label><input type="checkbox" id="marginnote-heman" class="margin-toggle" checked="" /><span class="marginnote">In fact, quite a few of Chong‚Äôs pieces at the retrospective brought a smile to my mind; it was a very pleasant visit for being thought-provoking on a number of levels. I do think his work deserves more local (Singaporean) appreciation (they are highly incisive commentaries, many with complex multi-layered abstractions of remarkable spareness that slowly unfurl in your mind; and are world-class with a Singaporean flavour to them).</span> and also immediately made me think of DLMs.</p>

<p><br />
<label for="marginfigure-heman" class="margin-toggle">‚äï</label><input type="checkbox" id="marginfigure-heman" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/IMG_5426.JPG" /><br />Call for the Dead, 2020,  Screenprint and acrylic on linen, Collection of the artist<br /><em>‚ÄúWhile on residency at STPI (Singapore Tyler Print Institute) in 2020, Heman Chong read and then redacted John le Carr√©‚Äôs first book, Call for the Dead‚Ä¶ Erasing everything except for its verbs, Chong‚Äôs Call for the Dead leaves us only with a sense of something having happened and the awareness that the text holds secrets not meant for us.‚Äù</em></span></p>
<figure><figcaption><span></span></figcaption><img src="/hankelvin.github.io/assets/img/IMG_5425.JPG" /></figure>

<p>The work (close-up in the image above, and a wide-shot of the hanging in the right margin) is titled <a href="https://www.stpi.com.sg/exhibitions/heman-chong-peace-prosperity-and-friendship-with-all-nations/" title="STPI -- Heman Chong: Peace Prosperity And Friendship With All Nations">‚ÄúCall for the Dead‚Äù</a> and is the text of a John le Carre novel that Chong had meticulously blacklined throughout to mask every single word except for the verbs. What immediately struck me was how there is a liminal quality to the work: at once filled with meaning yet poised for more ‚Äì the state of the text shown permits/carries potential, similar to being in the midst of complete masking/unmasking in the forward/reverse process. What also struck me as I got close to the work was how, despite there only being verbs remaining, I could still reasonably make out the narrative arc of the text (the numbers and lengths of the masked words did help in the decoding), and that brought to mind Davidsonian<label for="sidenote-davidson" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-davidson" class="margin-toggle" checked="" /><span class="sidenote">The view that sentence meaning is quantified over events denoted by verbs; verbs whose arguments are filled by the participants of the event; for e.g. the sentence <em>‚ÄúChong masked words from the book‚Äù</em> can be represented as: \(‚àÉe(mask(Chong, words, e)\) ‚àß \(from(words, book, e))\) or equivalently <em>‚ÄúThere exists some event \(e\) where a masking action by Chong on words takes place, and the words are from the book in \(e\)‚Äú</em>. See <a href="https://user.phil-fak.uni-duesseldorf.de/~filip/Davidson.67.pdf" title="Filip, Hana. _Lexical Semantics of Verbs: The Davidsonian event argument._ 2013">(Filip, 2013)</a> for a nice set of notes on Davidsonian event semantics. <br /><em>Sidenote:</em> this is a nice paper where visual question answering, scene graphs and Davidsonian semantics come together for interpretable verification of the contents of generated outputs <a href="https://arxiv.org/abs/2310.18235" title="Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation">(Cho et al, 2024)</a></span>  and neo-Davidsonian event semantics! This was also the inspiration for wondering in the previous post which set of word types a DLM would be most confident in unmasking at the start of the reverse process.<label for="sidenote_likely" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote_likely" class="margin-toggle" checked="" /><span class="sidenote">For conditional generation, a reasonable hypothesis is that this set might be a mix of verbs and proper nouns (i.e. entities in the context/prompts), but it would be interesting to verify this over a few Masked-DLMs to better understand what/how a DLM might be learning and generating.</span> Sometimes there can be connections between art, linguistics and computer science in quite beautiful ways.<label for="sidenote-calder" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-calder" class="margin-toggle" checked="" /><span class="sidenote">Another is Alexander Calder‚Äôs <a href="https://calder.org/works/hanging-mobile/untitled-c-1942-7" title="Untitled, 1942">mobiles</a>, whose elements hang in balance, suspended in place like constituent syntax trees of an utterance. And another is <a href="https://www.nationalgallery.sg/sg/en/learn-about-art/magazine/lin-hsin-hsin-speed-of-thought.html" title="Lin Hsin Hsin @ speed of thought">Lim Hsin Hsin</a> a trailblazing (on so many levels: female multi-disciplinary artist formally trained in mathematics and computer science practising from the 1970s in Singapore) artist whom I recently learned about at the latest iteration of the National Gallery‚Äôs <a href="https://www.nationalgallery.sg/sg/en/exhibitions/singapore-stories.html" title="Singapore Stories: Pathways and Detours in Art">permanent exhibition</a> of Singapore art.</span></p>

<h3 id="-2-masked-dlms-since-we-mask-randomly-in-the-forward-process-why-not-just-unmask-randomly-in-the-reverse-process-too">üé≤ 2. Masked-DLMs: Since we mask randomly in the forward process, <br />why not just unmask randomly in the reverse process too?</h3>
<p>We mask tokens randomly in the forward process for training Masked-DLMs, and this is what allows the DLM to learn so that we can do any-order/non-autoregressive generation in the reverse process. Therefore, it is natural to also think of doing the unmasking in the reverse process in a similarly random manner i.e. at each timestep \(t\), randomly choose a K-sized<label for="sidenote-k" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-k" class="margin-toggle" checked="" /><span class="sidenote">i.e. K = sequence length / number of denoising steps</span> set of still-masked index positions to be unmasked by the model <a href="https://proceedings.neurips.cc/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf" title="Structured Denoising Diffusion Models in Discrete State-Spaces">(Austin et al, 2021)</a>.</p>

<p>However, this is <ins>sub-optimal</ins>; especially if we follow the ‚Äúclassical‚Äù Masked-DLM formulation (i) where, once a token is unmasked it stays fixed in that state, and (ii) if we take large steps to unmask much more than a single token at each step, both of which would come together to give poorer generation quality. A key reason for this is because, when taking steps of \(\gt\) 1 tokens in the reverse process, <ins>the dependence of the tokens being unmasked are not taken into account</ins>, due to how Masked-DLM models are parameterised.<label for="sidenote-multidim" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-multidim" class="margin-toggle" checked="" /><span class="sidenote">For details, see the paragraph under the ‚ÄúMulti-dimension‚Äù subheader in ¬ß2.1 of <a href="https://arxiv.org/pdf/2406.03736" title="Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data">(Ou et al, 2024)</a> and ¬ß3.3 of <a href="https://arxiv.org/abs/2310.16834" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">(Lou et al, 2023)</a>.</span> Hence, if 100 tokens have to be unmasked at a step, it is quite possible that some of them could be incompatible with each other. The worst case scenario is when the incompatible tokens unmasked in the same step are located next to or close to each other in terms of sequence positions, which would leave little leeway for recovery in the later steps from the incompatibility. Such errors compound and could lead to increasingly less coherent text.</p>

<p>The most obvious solution for this would be to take small steps (and the safest would be to take a step of only one token at a time). Another easy solution could be to take smaller steps initially to try to avoid early irreversible clashes before increasing the step size over time. However these simple fixes are inadequate,<label for="sidenote-simple" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-simple" class="margin-toggle" checked="" /><span class="sidenote">The former entirely removes a major appeal of DLMs, which is its potential for ‚Äúparallel decoding‚Äù to achieve faster generation speed; whereas the latter does not fully address the token independence issue in a principled manner ‚Äì it would surface at later larger steps and hence could still lead to issues with generation quality.</span> and the following are two more sophisticated directions that have been proposed:</p>

<p>‚óºÔ∏è <ins><em>top-k confidence:</em></ins> Since Masked-DLMs parameterise the probability distribution over the vocabulary for each sequence position (\(0 \leq i \leq L\), where \(L\) is the length of the sequence to be generated), it makes sense to adopt a ‚Äútop-k‚Äù strategy over the model‚Äôs highest confidence at each position.<label for="sidenote-topk" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-topk" class="margin-toggle" checked="" /><span class="sidenote">Note that this is different from the top-k sampling in AR-LLMs, where for a given token to be generated it is sampled from the set of top-k tokens with the highest probabilities; here it refers to the top-k sequence positions with the highest probability values in their distributions.</span> This was used in <a href="https://arxiv.org/pdf/2302.05737" title="A Reparameterized Discrete Diffusion Model for Text Generation">(Zheng et al, 2024)</a> and recently systematically studied in <a href="https://arxiv.org/abs/2502.06768" title="Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions">(Kim et al, 2025)</a>. However, the top-k strategy loses its effectiveness in instances where the model has similar confidence (i.e. uncertainty) at some positions, and to address that <a href="https://arxiv.org/abs/2502.06768" title="Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions">(Kim et al, 2025)</a> proposed ‚Äútop-k margin confidence‚Äù.<label for="sidenote-topkm" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-topkm" class="margin-toggle" checked="" /><span class="sidenote">Here, the margin of the top-2 most probable tokens at each position is computed and is used to select the top-k subset for unmasking. This approach leaves out sequence positions that may have a high probability value in it, but where the model has uncertainty between its most probable tokens.</span> This approach has been shown to improve generation quality meaningfully and these methods have been incorporated as options in the LLaDA <a href="https://arxiv.org/abs/2502.09992" title="Large Language Diffusion Models">(Nie et al, 2024)</a> and Dream <a href="https://hkunlp.github.io/blog/2025/dream" title="Dream 7B"> (Ye et al, 2025)</a> scripts.</p>

<p>‚óºÔ∏è <ins><em>allowing corrections:</em></ins> Another direction is to allow corrections to be made to the already unmasked tokens, instead of keeping them fixed entirely. The corrections could come (i) via remasking, or (ii) using a setup with a specialised model.</p>

<blockquote>
  <p>‚óΩÔ∏è The former, also referred to as ‚Äúforward-backward‚Äù correction, involves picking a subset of the already unmasked tokens ‚Äì early approaches picked these by randomly sampling ‚Äì and returning (hence the ‚Äúbackward‚Äù) them to the masked state. Doing so allows another shot at unmasking them to appropriate tokens, since the model can now take into consideration the other tokens already unmasked (including the ones in the same step as it). Notably, it has been shown by <a href="https://arxiv.org/abs/2503.00307" title="Remasking discrete diffusion models with inference-time scaling">(Wang et al, 2025)</a> that it is not necessary for special training procedures to use such remasking.<label for="sidenote-remdm" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-remdm" class="margin-toggle" checked="" /><span class="sidenote">Their work outline a set of proofs showing that an already trained Masked-DLM can be used with remasking in a theoretically-supported manner; subject to the Masked-DLM having been trained with a negative ELBO tighter than the one for ReMDM that they specify (see ¬ß3.2 of their paper).</span> Note also that remasking permits a reverse process with more steps than the sequence length \(L\), which has analogues to test-time scaling in AR-LLMs <a href="https://arxiv.org/abs/2408.03314" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters">(Snell et al, 2024)</a>.<label for="sidenote-tts" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-tts" class="margin-toggle" checked="" /><span class="sidenote">Without remasking, i.e. masked tokens stay masked, hence once all of the sequence positions are unmasked, further steps have no effect. With remasking, this constraint is removed and can be seen as a form of test time-scaling for DLMs; this shares parallels with how additional generation steps in AR-LLMs, for e.g. ‚Äúthink‚Äù tokens (that do not count towards the answer) have been shown to improve AR-LLMs‚Äô answer performance.</span> The drawbacks with remasking are that: (i) using random sampling for tokens to remask, or ‚Äúuninformed correction‚Äù, is not optimal as it does not directly target the tokens that need correction and could even miss them<label for="sidenote-informed" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-informed" class="margin-toggle" checked="" /><span class="sidenote">To this, <a href="https://arxiv.org/abs/2503.00307" title="Remasking discrete diffusion models with inference-time scaling">(Wang et al, 2025)</a> also examined a few ways for selecting the tokens to be remasked (‚Äúinformed correction‚Äù), including a customisable approach (<em>ReMDM-conf</em>) that is (1) based on the confidence the model had when unmasking a token, and (2) which can be activated at points of the reverse process where remasking is most helpful.</span>, and (ii) remasking does add inference overhead<label for="sidenote-overhead" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-overhead" class="margin-toggle" checked="" /><span class="sidenote">Given some fixed number of tokens to unmask per-step, setting a portion to remask at each step increases the number of steps required.</span>.</p>
</blockquote>

<blockquote>
  <p>‚óΩÔ∏è On the other hand, the second approach such as the one proposed by <a href="https://arxiv.org/abs/2407.21243" title="Informed Correctors for Discrete Diffusion Models">(Zhao et al, 2024)</a>, involve a separate model that can be designed to identify and predict for the direct transitioning of certain unmasked tokens to another non-mask token. The appeal here is that this does away with having to do the superfluous transition to the mask token first, before transitioning to another prediction for the token. However, needing a separate model comes with the drawback that it adds to <strong>both</strong> training and inference overhead.</p>
</blockquote>

<p>Nonetheless, it should be possible to combine these approaches for Masked-DLM generation without negative effects on quality, i.e. use top-k confidence to select which positions to unmask as well as allow for correction with remasking.</p>

<h3 id="Ô∏è-3-are-there-alternatives-to-independent-token-level-unmasking">üñáÔ∏è 3. Are there alternatives to independent token-level unmasking?</h3>
<p>Since large-step parallel decoding in Masked-DLMs is sub-optimal (due to the independence of the tokens unmasked at every such steps; see above) this acts as a limit to faster decoding (i.e. generating with fewer denoising steps). As such, some work have examined how to mitigate this, including two that share some similarities with <strong>draft-model based speculative decoding</strong> in AR-LLMs <a href="https://arxiv.org/abs/2211.17192" title="Fast inference from transformers via speculative decoding">(Leviathan et al, 2023)</a>,<label for="sidenote" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote" class="margin-toggle" checked="" /><span class="sidenote">Broadly, this approach involves using a small AR-LLM (of the same architecture but with fewer parameters hence faster to run; the ‚Äòdraft model‚Äô) to auto-regressively generate multiple (\(K\)) tokens ahead of the larger and more capable AR-LLM used to generate the output (‚Äòtarget model‚Äô). It involves putting the \(K\) draft tokens through the target model (any speed-up is due to this: by computing probabilities for multiple tokens in parallel instead of one at a time) and accepting the draft tokens up the token before the predictions (based on probabilities) of the draft and target LLM depart, and then generating +1 token from the target model; if there is no departure, all tokens are accepted. Generation then continues with the draft model proposing the next \(K\) tokens. <br /><em>Sidenote:</em> Here‚Äôs a nice <a href="https://research.google/blog/looking-back-at-speculative-decoding/" title="Looking back at speculative decoding">blog post</a> recently written by the authors of <a href="https://arxiv.org/abs/2211.17192" title="Fast inference from transformers via speculative decoding">(Leviathan et al, 2023)</a>; the AI search summaries on Google‚Äôs search results page are served with the help of speculative decoding! This vLLM <a href="https://blog.vllm.ai/2024/10/17/spec-decode.html" title="How Speculative Decoding Boosts vLLM Performance by up to 2.8x">blog post</a> is also a good read.</span> in the sense that all these approaches leverage two models (one larger and one usually smaller, more efficient one) to improve generation quality.</p>

<p>An example of this is <a href="https://arxiv.org/abs/2410.01949" title="Discrete Copula Diffusion">(Liu et al, 2024)</a>‚Äôs Discrete Copula Diffusion (<strong>DCD</strong>), an approach where the probabilities of a Masked-DLM (<a href="" title="">SEDD Absorb</a>) are augmented with that of a much smaller-sized AR-LLM <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="Language models are unsupervised multitask learners">(GPT-2 small)</a> adding information about the joint distributions between tokens. This ‚Äúenhanced‚Äù distibution enables better generation quality in the form of lower generative perplexity while allowing the model to take fewer denoising steps.<label for="sidenote-copula" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-copula" class="margin-toggle" checked="" /><span class="sidenote">This is inspired by the concept of a copula model in statistics that parameterises a joint distribution using information from known marginals; which is what we have with the per-step token-level independent distributions from the Masked-DLM. Note however, that there are many details in this work and it might be worth looking into these further, one of which is how an off-the-shelf pretrained AR-LLM (GPT-2 small) can provide the joint distibution information on noisy (masked) data.<br /><em>Sidenote:</em> I found this book by <a href="https://www.nowpublishers.com/article/Details/ECO-005" title="Copula Modeling: An Introduction for Practitioners">(Trivedi &amp; Zimmer, 2007)</a> helpful to get a primer on copula modeling.</span> Whilst this approach will require the Masked-DLM and supporting AR-LLM share the same tokenizer, this is supported by the works in the vein of <a href="" title="Dream 7B: Diffusion Large Language Models">Dream (Ye et al, 2025)</a> and <a href="https://arxiv.org/abs/2508.15487" title="Scaling Diffusion Language Models via Adaptation from Autoregressive Models">DiffuLlama (Gong et al, 2025)</a> for converting AR-LLMs to Masked-DLMs.</p>

<p><br /></p>
<figure><figcaption><span>Image: Discrete Copula Diffusion ‚Äì source: <a href="https://arxiv.org/abs/2410.01949" title="Discrete Copula Diffusion">(Liu et al, 2024)</a></span></figcaption><img src="/hankelvin.github.io/assets/img/dcd.png" /></figure>

<p>Another is <a href="https://openreview.net/forum?id=sL2F9YCMXf" title="Energy-Based Diffusion Language Models for Text Generation">(Xu et al, 2025)</a>‚Äôs Energy-based Diffusion Language Model (<strong>EDLM</strong>), which shares some similarities in the use of a separate energy-based model (EBM) for obtaining information about token dependencies; and one of two ways they propose to obtain such EBMs is similarly through the use of AR-LLMs. <em>Tangentially related here is work such as <a href="https://aclanthology.org/2025.naacl-long.601.pdf" title="Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion">(Christopher et al, 2025)</a> that go in the other direction, that is to use DLMs to help speculative decoding for AR-LLMs.</em></p>

<h3 id="-4-the-sequence-lengths-have-to-be-fixed-and-caching-cant-be-done">üìê 4. The sequence lengths have to be fixed and caching can‚Äôt be done?</h3>

<p>Another limitation of initial Masked-DLMs (and all DLM variants for that matter) is that generation length is a hyperparameter that has to be set in advance (i.e. some length needs to be specified, which is then used to set the noised input, and then the denoising steps can take place over it). This hard constraint is very limiting; for outputs that turn out to be shorter in length, this leads to wasted computation. More significantly, if the ideal output happens to require more tokens than the length than specified, this results in reduced generation quality (through truncation, or via throwing off coherence as the model contorts to denoise within the specified length). At the same time, another issue relates to how the standard KV caching used in AR-LLMs<label for="sidenote-kvcache" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-kvcache" class="margin-toggle" checked="" /><span class="sidenote">See this excellent post by <a href="https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms" title="Understanding and Coding the KV Cache in LLMs from Scratch">Sebastian Raschka</a></span>, that helps speed generation there, is not directly transplantable to DLMs (due to how tokens can be unmasked anywhere across the sequence at each step, and how the state of the entire sequence at step \(t\) is needed for predicting the distributions of the following denoising step).</p>

<p>There are a number of approaches proposed to address the former (via variable length decoding) as well as the latter (through block-style diffusion). I highlight two recent pieces of work that combine them: (1) <strong>BD3-LMS</strong> proposed by <a href="https://openreview.net/forum?id=tyEyYT267x" title="Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models">(Arriola et al, 2025)</a>, and <strong>Fast-dLLM</strong> proposed by <a href="https://arxiv.org/abs/2505.22618" title="Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding">(Wu et al, 2025)</a>. Both introduce a notion of semi-autoregressive generation using blocks; each block is autoregressively processed up to the point where an end-of-sequence token is generated, and within each block the typical diffusion denoising steps are carried out (see GIF below). For fast-DLLM, they retain the typical DLM bidirectional attention across all blocks during generation and therefore have to rely on approximations for the caching, approximations which they support with analysis of the typical attention patterns across denoising steps. Notably, <a href="https://arxiv.org/abs/2505.16933" title="LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning">LLaDA-V</a> ‚Äì the multi-modal (vision &amp; language) version of LLaDA ‚Äì has fast-DLLM integrations built into its <a href="https://github.com/ML-GSAI/LLaDA-V">implementation</a>. On the other hand, the authors of BD3-LMS use a special attention mask that allows attention to be causal across blocks, which then allows them to cache the KV computation of earlier blocks (akin to how it are done in AR-LLMs at token level); as a result their approach does not need to rely on approximate caches, which in theory should guarantee better results than fast-DLLM.</p>

<p>~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~</p>
<figure><figcaption><span></span></figcaption><img src="/hankelvin.github.io/assets/img/bd3lms-ar.gif" /></figure>
<p>~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~</p>
<figure><figcaption><span></span></figcaption><img src="/hankelvin.github.io/assets/img/bd3lms-mdlm.gif" /></figure>
<p>~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~</p>
<figure><figcaption><span>Image: comparing auto-regressive generation with AR-LLMs (top), vanilla DLM denoising (middle), and BD3-LMS‚Äô semi-autoregressive generation (bottoms) ‚Äì source: <a href="https://m-arriola.com/bd3lms/" title="Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models">(Arriola et al, 2025)</a></span></figcaption><img src="/hankelvin.github.io/assets/img/bd3lms-sar.gif" /></figure>
<p>~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~</p>

<h3 id="-4-whats-next">üëâ 4. What‚Äôs next?</h3>
<p>This post went into a few of the key issues and considerations faced in the generation (denoising) process of Masked-DLMs, including looking at (i) how informed denoising strategies is used to help improve generation quality, (ii) how step-wise token-level independence is a limitation for faster generation speed together with some of the (early) ways proposed to address this, and (iii) how block diffusion approaches such as BD3-LMS enable variable length generation as well as KV caching similar to what‚Äôs done in AR-LLM for efficient LLM inference (which are particularly important for long sequence generation with Masked-DLMs). In the next post, I will take a look into reinforcement learning (RL) for masked-DLMs, covering what the key challenges there are, followed by recent proposals for addressing them; the focus will be on policy-gradient approaches similar to PPO and GRPO, that have recently been instrumental in AR-LLM post-training.</p>



    </article>
    <span class="print-footer">Diffusion Language Models -- Part Three (Generating with DLMs; through some art & linguistics) - September 1, 2025 - Kelvin Han</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <!-- <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>     -->
    
      <li>
        <a href="//www.linkedin.com/in/kelvin-han"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="//www.x.com/kelvinhan"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//scholar.google.com/citations?user=AuOeIe8AAAAJ&hl=en"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//www.github.com/hankelvin"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2025 &nbsp;&nbsp;KELVIN HAN</span></br> <br>
<span>This site was created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte-Jekyll</a> theme in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
