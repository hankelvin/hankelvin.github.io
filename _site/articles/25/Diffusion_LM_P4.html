<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Diffusion Language Models -- Part Four (Post-training with Reinforcement Learning)</title>
  <meta name="description" content="⊕Source: PNGs in GIF generated with ChatGPT.Recently, the post-training of large language models (LLMs) with reinforcement learning (RL) has been an importan...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
<!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  


  <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/print.css" media="print"> -->

  <link rel="canonical" href="http://localhost:4000/hankelvin.github.io/articles/25/Diffusion_LM_P4">

  <link rel="alternate" type="application/rss+xml" title="Kelvin's blog" href="http://localhost:4000/hankelvin.github.io/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<a href="/hankelvin.github.io/"><img class="badge" src="/hankelvin.github.io/assets/img/L1016424.png" alt="CH"></a>
	
		
  	
		
		    
		      <a href="/hankelvin.github.io/">Posts</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/page/">Stuff</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/css/print.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/robots.txt"></a>
		    
	    
  	
		
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/latex.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/main.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/sitemap.xml"></a>
		    
	    
  	
	</nav>
</header>
    <article class="group">
      <h1 style="max-width: 50%; line-height: 1.5">Diffusion Language Models -- Part Four (Post-training with Reinforcement Learning)</h1>
<p class="subtitle">October 4, 2025</p>

<p><label for="marginfigure-rlhf-good" class="margin-toggle">⊕</label><input type="checkbox" id="marginfigure-rlhf-good" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/rlhf.gif" /><br />Source: PNGs in GIF generated with ChatGPT.</span></p>

<p><span class="newthought">Recently, the post-training of large language models (LLMs) with reinforcement learning</span> (RL) has been an important source for the significant progress we are seeing in LLM capabilities (for reasoning, agents/tool-use, planning etc).</p>

<p><label for="marginnote-posttrain" class="margin-toggle"> ⊕</label><input type="checkbox" id="marginnote-posttrain" class="margin-toggle" checked="" /><span class="marginnote">The <em>post-training</em> of an LLM comes after <em>pretraining</em> (which is when LLMs are trained on next-token prediction over web-scale text). It “polishes” the LLM into the useful models we are used to interacting with. This <em><a href="https://tokens-for-thoughts.notion.site/post-training-101" title="Post-training 101: A hitchhikers guide into LLM post-training">blog post</a></em> by two Meta Super Intelligence (MSL) researchers gives a good overview of the post-training phase. Much of what is in their blog post would also apply to post-training for diffusion language models (DLMs).</span></p>

<p>This became especially apparent earlier this year when DeepSeek surprised (and <a href="https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html" title="Nvidia drops nearly 17% as China’s cheaper AI model DeepSeek sparks global tech sell-off">moved markets</a>) with the release of their R1 model <a href="https://www.nature.com/articles/s41586-025-09422-z" title="DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning">(Guo et al, 2025)</a>, an auto-regressive LLM (AR-LLM). Their model was post-trained with an efficient RL algorithm (GRPO, see below) in a way that unlocked “thinking” for improved performance on reasoning tasks.<label for="sidenote-reasoning" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-reasoning" class="margin-toggle" checked="" /><span class="sidenote">The term ‘reasoning’ with respect to LLMs, also referred to as Large Reasoning Models or LRMs, is still being settled upon. However, there are notable differences between the “thinking” traces produced by LRMs and what we might generally accept as reasoning by humans. A good overview of this can be found in <a href="https://arxiv.org/pdf/2504.09762v1" title="(How) Do reasoning models reason?">(Kambhampati et al, 2025)</a> and this <em><a href="https://x.com/rao2z/status/1966969679739768982" title="">list</a></em>.</span> Prior to this, however, RL post-training was already crucial for aligning AR-LLM generations towards users’ preferred forms/styles of text and conversation, as well as for meeting safety and security requirements.</p>

<p>In this post, I examine similar RL methods for diffusion language models (DLMs); which will be key for pushing DLMs to parity (or more) with existing AR-LLMs in terms of capabilities. In line with the previous posts of this series, I will focus on Masked-DLMs (which are the keenest focus of current research); on the RL side, my focus will be on online policy-gradient algorithms,<label for="sidenote-policy" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-policy" class="margin-toggle" checked="" /><span class="sidenote">Policy-gradient algorithms are named for how: (i) the <span style="color: red">policy</span> (the mechanism for generating <span style="color: red">trajectories</span> i.e. sequences of tokens when used in the context of LLM RL post-training) is in the form of a parameterized model, and (ii) the policy’s parameters are learned by following the gradient of some function with respect to those parameters. (<em>The definitions of the terms in <span style="color: red">red</span> can be found <a href="#terminology">below</a>.</em>) They can be categorised as being <em>online</em> or <em>offline</em>, where the “on”/”off” relates to whether the policy is learning from trajectories coming from itself (“on”) or not (“off”; e.g. from another model’s distribution). <br /><br /><ins><em>Sidenote:</em></ins> offline methods, such as DPO <a href="https://arxiv.org/abs/2305.18290" title="Direct Preference Optimization: Your Language Model is Secretly a Reward Model">(Rafailov et al, 2023)</a>, were instrumental for aligning LLM to human preferences (e.g. used in the training for Llama 3 <a href="https://arxiv.org/pdf/2407.21783" title="The Llama 3 Herd of Models">(Llama Team, AI @ Meta, 2024)</a> models). For the interested, similar methods have been proposed for diffusion models: e.g. VRPO used to post-train the original LLaDA Masked-DLM <a href="https://arxiv.org/abs/2502.09992" title="Large Language Diffusion Models">(Nie et al, 2025)</a> to give LLaDA 1.5 <a href="https://arxiv.org/abs/2505.19223" title="LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models">(Zhu et al)</a>, as well as ones for continuous diffusion such as Diffusion-DPO <a href="https://arxiv.org/abs/2311.12908" title="Diffusion model alignment using direct preference optimization">(Wallace et al, 2023)</a> and DSPO <a href="https://openreview.net/forum?id=xyfb9HHvMe" title="Direct Score Preference Optimization for Diffusion Model Alignment">(Zhu et al, 2025)</a>.</span> namely: Group Relative Policy Optimization (<strong>GRPO</strong>) <a href="https://arxiv.org/abs/2402.03300" title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models">(Shao et al, 2024)</a>, <a href="https://arxiv.org/abs/2501.12948" title="DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning">(DeepSeek AI, 2025)</a> and Proximal Policy Optimization (<strong>PPO</strong>) <a href="https://arxiv.org/abs/1707.06347" title="Proximal Policy Optimization Algorithms">(Schulman et al, 2017)</a>, which (i) are being used in post-training AR-LLM today; and (ii) have been found to reach better performance compared to offline algorithms <a href="" title="Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback">(Ivison et al, 2024)</a>.</p>

<p>The outline of this post is as follows: I will start by setting the scene with <em><a href="#-1-paint-a-picture-of-policy-gradient-rl-in-2-minutes">1. an accessible introduction to RL post-training using policy-gradient algorithms</a></em>, followed by outlining <em><a href="#️-2-can-we-just-reuse-the-llm-rl-post-training-methods-that-worked">2. the main challenge for DLM post-training with such methods</a></em>. I will then highlight <em><a href="#-3-what-policy-gradient-rl-methods-have-been-proposed-for-dlms">3. some proposed approaches for RL post-training of DLMs</a>. If you are familiar with RL post-training for LLMs, you could just skip directly to <a href="#️-2-can-we-reuse-the-rl-post-training-methods-that-worked-for-ar-llms">section 2</a>. Otherwise, to fully benefit from this post, going through my earlier posts <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P1">Part 1</a>, <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P2">Part 2</a> and <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P3">Part 3</a> for some background to DLMs first would probably be useful.</em></p>

<h3 id="️-1-paint-a-picture-of-rl-post-training-of-llms-in-5-minutes">🖼️ 1. Paint a picture of RL post-training of LLMs in 5 minutes?</h3>
<p>Before discussing policy-gradient RL for DLMs, let’s get to some common ground with an introduction to such methods, as well as a sense of how we are using them with AR-LLMs currently. I always find analogies help us to better grasp complex topics, so let’s start with one:</p>

<p><span style="color: #000080; font-family: Candara">Imagine you are a parent of a child Jesse, and you want them to learn to give the right answer (let’s refer to this as <em><strong>o</strong></em>, for output) to this question: <em>“Levy has two apples in his pocket, Alex has two apples in her bag. They have a picnic and eat one of the apples. How many apples do they have left?”</em> (essentially: <em>“What is 2+2-1 equals to?”</em>); let’s refer to this question as <strong>\(q_{k}\)</strong>. The idea is that it is best to have Jesse give <em>“3”</em> (or similar) as the final answer whenever they encounter <strong>\(q_{k}\)</strong> (or a similar problem). One way to help Jesse learn this could be to (i) pose <strong>\(q_{k}\)</strong> to Jesse multiple times, then (ii) have Jesse give an answer each time (let’s call each of this <strong>\(o^{k}_{i}\)</strong>), and then (iii) tell Jesse for each <strong>\(o^{k}_{i}\)</strong> whether it is a good answer.</span></p>
<div style="background-color: #249ae9ff; max-width: 50%; color: white; padding: 20px; border-radius: 8px; margin: 10px;">
  <h3 style="margin: 0 0 15px 0; width: 100%;font-family: Candara">Some possible answers of Jesse's</h3>   
  <p style="margin: 0; width: 100%; text-align: left; font-size: 16px; font-family: Candara">   
  💬 <em>o</em><sub>1</sub>: I know this, the answer is 3! 
  <br />
  💬 <em>o</em><sub>2</sub>: I don't know, the answer is 3? 
  <br />
  💬 <em>o</em><sub>3</sub>: I love chicken nuggets! I will never eat apples!
  <br />
  💬 <em>o</em><sub>4</sub>: Levy has 2 apples and they eat 1 so there has to be 3 apples left. 
  <br />  
  💬 <em>o</em><sub>5</sub>: Three!
  <br />  
  💬 <em>o</em><sub>6</sub>: They have 2 plus 2 apples so that is 4 apples. They eat one, so 4 minus 1, that means they have 3 apples left. Duh!
  </p>
</div>

<p>We can also see from above that some of the answers that Jesse might come up with could be better than others; in terms of correctness (in the final answer, and in the reasoning) as well as for style.<label for="sidenote-Jesse" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-Jesse" class="margin-toggle" checked="" /><span class="sidenote">Some answers are clearly off (i.e. <em>o</em><sub>3</sub>). Some (i.e. <em>o</em><sub>4</sub>) give the correct final answer, but have a wrong reasoning process for getting to it. Some might be nearly identical, but one amonsgt them is preferred over another, e.g. <em>o</em><sub>1</sub> versus <em>o</em><sub>2</sub> (for a more confident Jesse). Others might be quite different yet slightly preferred over another, e.g. <em>o</em><sub>5</sub> versus <em>o</em><sub>6</sub> (depending on whether we prefer to have a Jesse that will give a reasoning along with their answer, but in a sassy way).</span> Therefore, we can expect to have some preferences between each <strong>\(o^{k}_{i}\)</strong>, and hence we might want to steer Jesse’s mind such that whenever Jesse encounters <strong>\(q_{k}\)</strong>, ideally Jesse gives <strong>\(o^{k}_{i}\)</strong> that is most preferable.</p>

<p><ins>In essence, what we want to do for Jesse is similar to what we want to do with LLMs using policy-gradient RL post-training!</ins> i.e. we want an LLM to learn, by updating its parameters, that when presented with a certain prompt <strong>\(q_{k}\)</strong> (or similar) it should generate responses that are preferred (achieve highest reward). This is done via getting the LLM to give higher likelihoods for the sequence of tokens in the higher-scoring <strong>\(o^{k}_{i}\)</strong>.</p>

<div style="background-color: #AFEEEE; max-width: 50%; color: black; padding: 20px; border-radius: 8px; margin: 10px;" id="terminology">
  <h3 style="margin: 0 0 15px 0; width: 100%;">Some terminology</h3>   
  <p style="margin: 0; width: 100%; text-align: left; font-size: 16px;">
  Before proceeding, let's set the definitions of some key RL terms first. Each of these terms are also associated with concepts (in brackets and <span style="color: #000080">blue</span> below) from the Jesse example, so as to connect them with RL on AR-LLMs.</p>
  <br />
  <p style="margin: 0; width: 100%; text-align: left; font-size: 16px;">   
  ▪️ <span style="color: red">"state"</span>: information about the current situation at a given moment in time; 
  <br />
  ▪️ <span style="color: red">"action"</span>: a decision/choice that can be taken at the point of a certain state; 
  <br />
  ▪️ <span style="color: red">"trajectory"</span>: a sequence of states and actions that can be taken (<span style="color: #000080"><em>o<sup>k</sup><sub>i</sub></em></span>); 
  <br />
  ▪️ <span style="color: red">"policy"</span>: some model that can give us trajectories (<span style="color: #000080">Jesse</span>); 
  <br />
  ▪️ <span style="color: red">"reward"</span>: feedback on a trajectory, i.e. what can be gotten if the trajectory is taken (<span style="color: #000080">whether <em>o<sup>k</sup><sub>i</sub></em> is good or bad/how good or how bad</span>); 
  <br />
  ▪️ <span style="color: red">"reward model"</span>: some method/model giving the reward for a trajectory (<span style="color: #000080">you!</span>). 
  <br />
  ▪️ <span style="color: red">"advantage"</span>: how much better taking action <em>a<sub>t</sub></em> at state <em>s<sub>t</sub></em> is compared to the average of all actions possible. 
  <br />
</p>
</div>

<p><label for="marginnote-example" class="margin-toggle"> ⊕</label><input type="checkbox" id="marginnote-example" class="margin-toggle" checked="" /><span class="marginnote">To make the definitions more concrete let’s shift the example with Jesse above to an AR-LLM: Let’s say we are at the point in time (<em>state</em>) where the AR-LLM has just processed the prompt \(q_{k}\) fed to it. Let’s call this state \(s_{0}\). For the sake of this example, let us assume that the AR-LLM can only ever give answers to \(q_{k}\) from the 6 examples above (i.e. <em>o</em><sub>1</sub> to <em>o</em><sub>6</sub>). If we prefer <em>o</em><sub>6</sub> the most, then the <em>action</em> we want from the AR-LLM immediately after \(s_{0}\) is to return the word “They” <em>(in the next-token prediction set-up of AR-LLMs, this means striving to give this word the highest probability)</em>. The objective is to have the AR-LLM learn to return a sequence (i.e. <em>trajectory</em>) of state-action decisions so as to give an answer that obtains as high a reward as possible. Note that the learning for the policy also involves cases such as these: if the action chosen was to return “I” after \(s_{0}\), then the AR-LLM should learn that at such \(s_{1}\), the word “know” should have the highest probability (applies if we prefer <em>o</em><sub>1</sub> over all the other answers (<em>o</em><sub>2</sub> and <em>o</em><sub>3</sub>) that start with “I”). and so on and so forth…</span></p>

<p>In practice, we achieve this by getting the LLM (the policy) to generate a diverse set of answers for a given prompt \(q_{k}\) by using a sufficiently high sampling <a href="https://huggingface.co/blog/how-to-generate#:~:text=the%20so%2Dcalled-,temperature,-of%20the%20softmax" title="How to generate text: using different decoding methods for language generation with Transformers">temperature</a>. The LLM learns via the feedback from the rewards of different experiences (i.e. pairs of <strong>\(q_{k}, o^{k}_{i}\)</strong>) which is the best answer to give.</p>

<p style="size: 22pt; font-weight: bold;">PPO and GRPO briefly: efficient &amp; stable training</p>
<p>(<em>Bear with me, just a little more common ground… 😅, so that we can situate the next section properly.</em>) In this section, I zoom in to focus on two aspects shared by the PPO and GRPO algorithms; a sense of these aspects are necessary for me to be able to explain the key points of the subsequent sections.<label for="sidenote-fuller" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-fuller" class="margin-toggle" checked="" /><span class="sidenote">I give a very general view here, but there is a fair bit more behind both algorithms; for a fuller understanding of them take a look at the following resources to start: <a href="https://yugeten.github.io/posts/2025/01/ppogrpo/" title="A vision researcher’s guide to some RL stuff: PPO &amp; GRPO">this post by Jimmy Shi</a>, <a href="https://rlhfbook.com" title="Reinforcement Learning from Human Feedback">this series by Nathan Lambert</a> and <a href="https://huggingface.co/blog/deep-rl-ppo" title="Proximal Policy Optimization (PPO)">this HuggingFace RL course unit</a>.</span></p>

<p>▪️ A major preoccupation for RL training in general (i.e. including PPO/GRPO) is to find some balance between <strong>exploration</strong> (i.e. generating diverse answers to receive useful feedback for learning) and <strong>exploitation</strong> (i.e. leveraging useful knowledge the policy has learned from past encounters, e.g. from Jesse’s <em>o</em><sub>5</sub> which gets a good reward).<label for="sidenote-stability" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-stability" class="margin-toggle" checked="" /><span class="sidenote">The trade-off is as follows: ▪️ allowing more exploration (i.e. via generating the <strong>\(o^{k}_{i}\)</strong> trajectories by sampling with high temperature) results in very sparse signals (to go the extreme: imagine that for every <strong>\(q_{k}\)</strong>, we have to generate all the possible combinations of words in English almost all of which would have very low reward with respect to <strong>\(q_{k}\)</strong>) and wastes compute; whereas, on the other hand, ▪️ relying on already learned knowledge (e.g. generating <strong>\(o^{k}_{i}\)</strong> by sampling with low temperature) may keep the policy around poor/sub-optimal outputs i.e. does not allow it to reach an optimal <strong>\(o^{k}_{i}\)</strong>.</span> When applying PPO/GRPO to AR-LLMs, the bottleneck is the generating of trajectories (due to the generation process being auto-regressive) and it typically takes up most of the training run-time. Hence, it is typical to reuse the same set of sampled trajectories for a few more update steps<label for="sidenote-mu" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-mu" class="margin-toggle" checked="" /><span class="sidenote">This is <em>“K epochs”</em> in Algorithm 1 of the PPO paper <a href="https://arxiv.org/abs/1707.06347" title="Proximal Policy Optimization Algorithms">(Schulman et al, 2017)</a> and <code class="language-plaintext highlighter-rouge">num_ppo_epochs</code> in the <a href="https://huggingface.co/docs/trl/main/en/ppo_trainer#trl.PPOConfig">TRL implementation</a>; the \(\mu\) hyperparameter in Algorithm 1 of the DeepSeek Math (GRPO) paper <a href="https://arxiv.org/abs/2402.03300" title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models">(Shao et al, 2024)</a> and <code class="language-plaintext highlighter-rouge">num_iterations</code> in the <a href="https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOConfig">TRL implementation</a>.</span> to squeeze more learning out of them. <em>Hereon, I will use the term <strong>\(\mu\)-updates</strong> to refer to these update steps.</em> Think of it in this way: although going through one round of (\(q_{k}, o^k_1... o^k_6\)) with Jesse might help them get a little closer to giving the most preferred output, but it might not be sufficient… so we repeat with multiple rounds of (\({k}, o^k_1... o^k_6\)) to help Jesse learn.<label for="sidenote-onoff" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-onoff" class="margin-toggle" checked="" /><span class="sidenote"><ins><em>Sidenote:</em></ins> While PPO and GRPO are recognised as online methods, a case could be made that these subsequent \(\mu\)-updates after the first step/epoch, are at least <em>slightly off-policy</em> <a href="https://arxiv.org/abs/2505.17508" title="On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning">(Zhang et al, 2025)</a>… especially when \(\mu\) is set to a large number.</span></p>

<p>▪️ Another major preoccupation (for policy-gradient methods in general) is achieving <strong>stable training</strong> to facilitate successful policy learning.<label for="sidenote-variance" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-variance" class="margin-toggle" checked="" /><span class="sidenote">Since we typically train across diverse problems \(q_k\) that each have their own reward distributions, this adds to the variance in the gradient estimates (which is already present between trajectories of a given \(q_k\)); therefore, when taking update steps, large updates can overfit the policy to some problems at the expense of others, leading to instability and hindering overall learning.</span> Hence, one of the design principles in PPO <a href="https://arxiv.org/abs/1707.06347" title="Proximal Policy Optimization Algorithms">(Schulman et al, 2017)</a> was to ensure stability across update steps. This was done by adding the following to the training objective of vanilla policy-gradient methods (e.g. REINFORCE <a href="https://dl.acm.org/doi/10.1007/BF00992696" title="Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning">(Williams, 1992)</a>): (i) a <strong>KL-regularisation</strong> term;<label for="sidenote-kl" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-kl" class="margin-toggle" checked="" /><span class="sidenote">The <em><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a></em> is a measure of how close/apart one distribution (\(P\)) is to another (\(Q\)); it is an asymmetric measure; so KL of \(P || Q\) is not the same as KL of (Q||P).</span> and (ii) the use of clipping as a floor/ceiling on the update. These help avoid updates to the policy that veer too far from some "trusted" zone of some reference policy that has already been established (for e.g. from explorations in previous updates, or an initial SFT-ed policy). Since GRPO is actually based upon PPO,<label for="sidenote-grpomods" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-grpomods" class="margin-toggle" checked="" /><span class="sidenote">Doing away with the need for a separate memory- and compute-heavy value model to assess advantage, replacing it with a group-based advantage estimation.</span> a similar objective to PPO can also be found there.<label for="marginfigure-ppogrpo" class="margin-toggle">⊕</label><input type="checkbox" id="marginfigure-ppogrpo" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/grpo.png" /><br />Image: PPO and GRPO; their similarities and differences – source: <a href="https://arxiv.org/abs/2402.03300" title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models">(Shao et al, 2024)</a>. Note that there are variants of PPO that permit generating multiple trajectories, computing their rewards and advantages in one pass (similar to the GRPO figure) but still needing a value model. </span></p>

<p><em>It is these – the <strong>KL regularisation term</strong> and the <strong>\(\mu\)-updates</strong> – that presents some challenges to overcome (as well as opportunities to leverage as in <strong>diffuGRPO</strong>) for the use of PPO/GRPO on DLMs, and we will discuss these next… (Note: The rest of this post will go into the weeds on these points and will be more technical.)</em></p>

<h3 id="️-2-can-we-reuse-the-ppogrpo-methods-that-worked-for-ar-llms">♻️ 2. Can we reuse the PPO/GRPO methods that worked for AR-LLMs?</h3>
<p>The short answer is… <span style="color: blue">broadly, yes</span> but with the need for some <span style="color: blue">non-trivial modifications</span> to address the issue of how to obtain the likelihood for trajectories from a Masked-DLM. These likelihoods are needed in two places in the PPO/GRPO objective: (i) for an <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a> weight, as well as (ii) an estimate for the KL-regularisation term.<label for="sidenote-grpo-obj" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-grpo-obj" class="margin-toggle" checked="" /><span class="sidenote">We use the GRPO objective to illustrate (<em>with clipping omitted to reduce clutter in the equation</em>): 
\(\begin{aligned}
L_{GRPO}(\theta) = - \frac{1}{\sum_{i=1}^{G} |o_i|} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|} 
\\
\bigg[ {\color{red}\frac{\pi_\theta(o_{i,t}|x, o_{i,&lt;t})}{{\left[ \pi_\theta(o_{i,t}|x, o_{i,&lt;t}) \right]}_{\text{no grad}}}} \hat{A}_{i,t}  {\color{blue} - \beta D_{\text{KL}}[\pi_\theta \| \pi_{\text{ref}}} ] \bigg]
\end{aligned}\)
<br />
where \(\pi_{\theta}\) is the current policy and \(\pi_{ref}\) is either the initial (typically obtained via SFT) or some earlier-update \(\pi_{\theta}\). 
<br />
As we can see:
<br />
▪️ the per-token likelihoods (obtained twice, once with gradients through the policy \(\pi_{\theta}\) and another without gradients) are used in the first term (in <span style="color: red">red</span>). The ratio of these corresponds to an <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a> on the advantages \(\hat{A}_{i,t}\) (to address that trajectories are coming slightly off-policy in the <strong>μ-updates</strong> steps);
<br />
▪️ the <em>KL-regularisation term</em> is in <span style="color: blue">blue</span>; and this is where the sequence-level likelihoods are used. In practice, this KL estimate is implemented via this form: <code class="language-plaintext highlighter-rouge">KL</code> \(= e^r - r - 1\) where \(r = log( \pi_{\theta}(o_{i,t}|x, o_{i,&lt;t}) / \pi_{ref}(o_{i,t}|x, o_{i,&lt;t}) )\). The per-token likelihood for \(\pi_{\theta}\) above can be reused, and only the ones from \(\pi_{ref}\) need to be computed here. For a concrete feel: see the implementation in <a href="https://github.com/huggingface/trl/blob/e086f073cf6dee30acc2d3fe357db21e1901c2be/trl/trainer/grpo_trainer.py#L1719">TRL</a>. 
<br /><br /><ins><em>Sidenote:</em></ins> Recent studies <a href="https://arxiv.org/abs/2505.17508" title="On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning">(Zhang et al, 2025)</a> and <a href="" title="On a few pitfalls in KL divergence gradient estimation for RL">(Tang et al, 2025)</a> have established that there are non-trivial differences relating to a set of fine-grained choices of the method and implementation for the KL divergence estimate. <span style="color: blue"><strong>Note that these have implications for online RL of DLMs due to the need to estimate these estimates there (see <em><a href="#dlm_estimate">below</a></em>). To my mind, these two pieces are recommended reading for RL on DLMs.</strong></span> 
<br /><br /><ins><em>Sidenote:</em></ins> If the beta (β) coefficient, which controls the amount of KL-regularisation in PPO/GRPO, is set to be zero, then there is no need for the sequence-level likelihoods. Empirically, there have been reports recently that the KL-regularisation may not be necessary for AR-LLMs (quite likely under certain training setups i.e. hyperparameter setting, modeling choice where the encountered KL divergences between \(\pi_{\theta}\) and \(\pi_{ref}\) are low). See for e.g. <a href="https://ai.meta.com/research/publications/cwm-an-open-weights-llm-for-research-on-code-generation-with-world-models/" title="CWM: An Open-Weights LLM for Research on Code Generation with World Models">(Copet et al, 2025)</a>; page 13 of paper.</span></p>

<p>Computing these likelihood for trajectories is easy for AR-LLMs because of how they factorise sequence probabilities at a token-level; i.e. at each step, the AR-LLM predicts from its vocabulary the most likely token to generate. As a result, it is very easy to compute what an AR-LLM thinks is the likelihood of any sequence of tokens (by chain-rule, i.e. simply summing the log-probabilities for each token of the sequence).<label for="sidenote-factorise" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-factorise" class="margin-toggle" checked="" /><span class="sidenote">See also footnote 27 in the <em><a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P2#:~:text=lower%20bound%20(ELBO)-,Unlike,-AR%2DLLMs%20which" title="Diffusion Language Models -- Part Two (What kinds are there and how is one trained?)">second post</a></em> of this series.</span></p>

<p>However, this is <span id="dlm_estimate">not the case 😵‍💫</span> for Masked-DLMs (and discrete diffusion models generally). Although we do get probabilities for tokens at each step of the diffusion generation process (which is what allows us to decide which token to unmask into), each of these steps is a denoising one <ins>that depends on all its preceding steps</ins>. In other words, computing sequence probabilities for DLMs require going through multiple denoising steps (from \(T\) to 0). To have to keep doing this for every sampled trajectory during online RL training with PPO/GRPO is very computationally expensive, and will be significantly worse for very long sequences.<label for="sidenote-efficient" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-efficient" class="margin-toggle" checked="" /><span class="sidenote">Although the efficient DLM methods I covered in the previous post (such as Block Diffusion <a href="https://openreview.net/forum?id=tyEyYT267x" title="Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models">(Arriola et al, 2025)</a>) can help alleviate this, the increase in computation required – compared to what is required in AR-LLMs – will still be substantial.</span> As such, there is a need to establish ways to efficiently, yet as accurately as possible, estimate these likelihoods with the DLM. This is the focus of much research currently and we will look into in the next section.</p>

<h3 id="-3-what-have-been-proposed-for-dlms">💡 3. What have been proposed for DLMs?</h3>
<p>This section outlines two research trends in online RL algorithms to Masked-DLMs. All of these started with <strong>diffuGRPO</strong> <a href="https://arxiv.org/abs/2504.12216?" title="d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning">(Zhao et al, 2025)</a>, which landed in Q1 this year and was the first work to explore a way of bringing online RL algorithms to Masked-DLMs. <strong>diffuGRPO</strong> and the initial wave of research is distinguished by their main contributions for ways to estimate likelihoods with Masked-DLMs, another more recent wave (released in the last month or so) begin to explore extensions for Masked-DLMs with semi-autoregressive generation for longer generations and with more efficiency (e.g. with KV caching).</p>

<p style="size: 22pt; font-weight: bold">Efficient likelihood estimation for online RL on Masked-DLMs</p>
<p>Each of the three pieces of work mentioned here proposed a way to do the likelihood estimation. Note that although they were formulated for GRPO, it should be possible to leverage their likelihood approaches for use in a PPO setup.</p>

<p>◼️ <span id="diffugrpo"><strong>diffu-GRPO</strong></span> <a href="https://arxiv.org/abs/2504.12216?" title="d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning">(Zhao et al, 2025)</a>: estimate the per-token likelihood of a trajectory by simply doing unmasking in one-step.<label for="sidenote-diffugrpo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-diffugrpo" class="margin-toggle" checked="" /><span class="sidenote">In practical terms, this is done as follows: for a given prompt \(q_{k}\) append it with a fully-masked continuation (i.e. max sequence generation length) and pass it through the Masked-DLM; this output is the estimated per-token probability distribution (conditioned by the prompt \(q_{k}\)).</span> As noted above, such one-/few-step unmasking does not reflect the multi-step denoising in Masked-DLM – hence and quite importantly, their proposal hinges on (i) the \(\mu\)-updates typically (but not mandatorily) used in GRPO, and (ii)  a random masking to the <em>prompt \(q_{k}\)</em> portion of the input (i.e. input = \(q_{k}\) + fully masked continuation). At every of the \(\mu\)-steps, the mask is randomised but always fixed at 15%.<label for="sidenote-d1masking" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-d1masking" class="margin-toggle" checked="" /><span class="sidenote">See Appendix A of paper: <em>“In gradient update iterations, each token in the prompt is randomly masked with a probability pmask = 0.15 for log-probability estimation.”</em> </span> We can see this as obtaining slightly varied likelihood estimates for a set of inputs closely resembling the prompt \(q_{k}\) which according to the authors <em>“acts a form of regularization for policy optimization”</em>. As for estimating sequence-level likelihood of a trajectory: the authors assume <a href="https://en.wikipedia.org/wiki/Mean-field_theory">mean-field decomposition</a> (i.e. a series of localized independent distributions can be useful for approximating a complex conditional distribution), allowing them to simply sum the trajectory’s per-token log probabilities to get this estimate. At least two pieces of empirical support are available for <strong>diffu-GRPO</strong>: (i)  <a href="https://arxiv.org/abs/2504.12216?" title="d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning">(Zhao et al, 2025)</a> reported consistently stronger performance on four different math and puzzle/planning logical problems;<label for="sidenote-diffugrpo-results" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-diffugrpo-results" class="margin-toggle" checked="" /><span class="sidenote">See Table 1 and Figure 5 of their <a href="https://arxiv.org/abs/2504.12216?" title="d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning">paper</a></span> and (ii) the same approach for obtaining per-token and sequence likelihood was adopted by <strong>IGPO</strong> <a href="https://arxiv.org/pdf/2509.10396" title="Inpainting-Guided Policy Optimization for DiffusionLarge Language Models">(Zhao et al, 2025b)</a> as well and tested successfully on reasoning benchmarks there.
<br /><br /></p>
<figure><figcaption><span>Image: likelihood estimation approach in <strong>diffuGRPO</strong> via one-step denoising (varied mask on prompt tokens across \(\mu\)-updates) – source: <a href="https://arxiv.org/abs/2504.12216?" title="d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning">(Zhao et al, 2025)</a>.<br /><br /></span></figcaption><img src="/hankelvin.github.io/assets/img/diffugrpo.png" /></figure>

<p>◼️ <strong>coupled-GRPO</strong> <a href="https://arxiv.org/abs/2506.20639" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation">(Gong et al, 2025)</a>: departs from <strong>diffu-GRPO</strong> in that they apply the masking to the continuation portion (i.e. the \(o^k_i\)) for <em>each trajectory</em>. They also use two samples on <em>each trajectory</em> at every \(\mu\)-step for the estimation (compared to <strong>diffu-GRPO</strong>’s use of only one sample of <em>each prompt \(q_{k}\)</em> at every \(\mu\)-step, which requires much less computation). Each of the two samples masks different, but paired (henced the “coupled” in the name) parts of the continuation.<label for="sidenote-coupled" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-coupled" class="margin-toggle" checked="" /><span class="sidenote">This involves sampling a random timestep \(t\), and then setting the other \(\hat{t}\) so that (i) \(t + \hat{t} = T\), the terminal timestep (i.e. 1.0). Then, what is masked for \(t\) is not masked in \(\hat{t}\) and vice-versa.</span> This ensures that (i) every token in a trajectory is involved once in the estimation giving <em>“each token a non-zero learning signal”</em>, and (ii) it also more closely mimics the denoising generation process in Masked-DLMs (where probabilities are produced on partially masked continuations). <strong>coupled-GRPO</strong> was used by <a href="https://arxiv.org/abs/2506.20639" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation">(Gong et al, 2025)</a> in their training for DiffuCoder, a code generation-focused Masked-DLM, who stated that <strong>coupled-GRPO</strong> was formulated in response to their findings that <strong>diffu-GRPO</strong>’s likelihood approximation methods do not <em>“yield a stable reward improvement…, probably because code tasks demand higher token-level generation accuracy than math tasks”</em>. They go on to show that <strong>coupled-GRPO</strong> leads to stronger performance through stabler rewards (see left and centre chart of Figure 7 in their paper) over <strong>diffu-GRPO</strong> for coding tasks, as well as for another baseline where they remove the masking coupling of (\(t, \hat{t}\)). Interestingly they also found that coupled-GRPO required sampling trajectories at a higher temperature for success (see right chart of Figure 7 in their paper) which has congruence with similar findings on online RL for AR-LLMs recently <a href="https://arxiv.org/abs/2505.24864" title="ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models">(Liu et al, 2025)</a>. 
<br /><br /></p>
<figure><figcaption><span>Image: likelihood estimation approach in <strong>Coupled-GRPO</strong> to balance coverage and reduce variance – source: <a href="https://arxiv.org/abs/2506.20639" title="DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation">(Gong et al, 2025)</a>.</span></figcaption><img src="/hankelvin.github.io/assets/img/coupled-grpo.png" /></figure>

<p>◼️ <strong>uni-GRPO</strong> <a href="https://arxiv.org/abs/2505.15809" title="MMaDA: Multimodal Large Diffusion Language Models">(Yang et al, 2025)</a>: was applied in the training procedure for MMaDA, a Masked-DLM with multi-modal (vision and text) capabilities. It is similar to <strong>coupled-GRPO</strong> in that it also masks on the continuation to obtain the likelihood estimates. Specifically, the noise for each \(\mu\)-step update is randomly sampled from a uniform distribution (instead of the fixed 15% of the prompt in <strong>diffu-GRPO</strong>, which also meant that the same timestep (i.e. \(T\)) across all samples was used there). Only one sample on <em>each trajectory</em> at every \(\mu\)-step is taken (i.e. more computation than <strong>diffu-GRPO</strong> but less than <strong>coupled-GRPO</strong>). In a departure from the other two approaches, the per-token likelihood is computed with the masked tokens (i.e. this relates to the ELBO of the Masked-DLM),<label for="sidenote-elbo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-elbo" class="margin-toggle" checked="" /><span class="sidenote">See Equation 3 of  the MMaDA paper for <strong>uni-GRPO</strong>; and compare with Equation 4 of the DiffuCoder paper for <strong>coupled-GRPO</strong>.</span> and the sequence level likelihood <em>“is then approximated by averaging over masked tokens”</em> (see Equation 4 in paper). Taking the ELBO as the estimate is quite a meaningful departure from the <strong>diffu-GRPO</strong> and <strong>coupled-GRPO</strong> approaches, and although it has a theoretical connection to the Masked-DLM training objective, it is not clear that it provides a better estimate for online RL training compared to the case where all tokens are considered (as in <strong>coupled-GRPO</strong>); nonetheless, it is clear that <strong>uni-GRPO</strong> outperforms <strong>diffu-GRPO</strong> (likely due to the larger-sized sampling, i.e. every trajectory every \(\mu\)-step).<label for="sidenote-compare" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-compare" class="margin-toggle" checked="" /><span class="sidenote">See comparisons of their performance in Figure 3 in §5.2 of the MMaDA paper and Table 1 of the IGPO paper that also leverages <strong>diffu-GRPO</strong>.</span></p>

<p style="size: 22pt; font-weight: bold">Other proposals for Masked-DLMs</p>

<p>◼️ <strong>wd1</strong> <a href="https://arxiv.org/abs/2507.08838" title="wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models">(Tang et al, 2025)</a>: proposes a few modifications to the GRPO objective which allow it to be used on a Masked-DLM with likelihood evaluation through one policy only (the current policy \(\pi_{\theta}\)). This is desirable as it is much more computationally efficient compared to the approaches above, which needed to do so for the policy before the \(\mu\)-update (\(\pi_{old}\)) and the reference policy (\(\pi_{ref}\)). Briefly, their approach hinges on shifting from (i) applying the importance sampling to the advantage (as per original PPO; see above) to (ii) applying a reverse KL-divergence penalty.<label for="sidenote-wd1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-wd1" class="margin-toggle" checked="" /><span class="sidenote">See §3.1 and Equation 3 of their <a href="https://arxiv.org/abs/2507.08838" title="wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models">paper</a>.</span> This enables their derivation of an expression on the GRPO objective that only needs likelihood estimates from \(\pi_{\theta}\).<label for="sidenote-wd1-obj" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-wd1-obj" class="margin-toggle" checked="" /><span class="sidenote">In order to obtain their expression of the GRPO objective, it also involves shifting the trajectory sampling (from \(\pi_{old}\)) to a geometric mixture of \(\pi_{old}\) and \(\pi_{ref}\) (see §3.1 of their paper).</span> They report obtaining up to 16% better performance over <strong>diffuGRPO</strong> on math and logic/puzzle planning benchmarks even without having to do an SFT phase (which is, on the other hand, needed for <strong>diffuGRPO</strong> to reach reasonable performance).<label for="sidenote-wd1-results" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-wd1-results" class="margin-toggle" checked="" /><span class="sidenote"><ins><em>Sidenote:</em></ins> Although, a case could be made that the settings <a href="https://arxiv.org/abs/2507.08838" title="wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models">(Tang et al, 2025)</a> use for comparison with <strong>diffuGRPO</strong> might not be fully like-for-like. Their wd1 objective is obtained assuming the \(\beta\)-controlled KL-regularisation term (see above) is included in the GRPO objective (see their Equation 5). While the final expression of the wd1 objective does away with the KL-regularisation term and does not include it explicitly, the controlling \(\beta\) term remains embedded throughout the wd1 objective (their Equations 9 and 6). Yet in practice \(\beta\) is set to 0 (see “Implementation” in §4 of their paper) for the model trained with wd1 in their experiments, in effect this leaves out the consideration of any KL-regularisation. On the other hand, the \(\beta\) from the original <strong>diffuGRPO</strong> paper (0.04) was kept (see Table 5 of Appendix B.4). Perhaps it will be helpful to also understand how <strong>diffuGRPO</strong> performs without the KL-regularisation applied.</span> Notably however, it does not appear that this approach leads to stronger empirical outcomes when compared with <strong>uniGRPO</strong>.<label for="sidenote-wd1-less" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-wd1-less" class="margin-toggle" checked="" /><span class="sidenote">Compare the reported scores on GSM8K and MATH500 by <a href="https://arxiv.org/pdf/2509.10396" title="Inpainting-Guided Policy Optimization for DiffusionLarge Language Models">(Zhao et al, 2025b)</a> (refer to Table 1) and <a href="https://arxiv.org/abs/2507.08838" title="wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models">(Tang et al, 2025)</a> (refer to Table 3; look at the 256-length results as the other paper used a 256 length setting – see Appendix A there). </span></p>

<!-- Note however, a slight discrepancy: the __diffuGRPO__ results reported by <a href='https://arxiv.org/pdf/2509.10396' title='Inpainting-Guided Policy Optimization for DiffusionLarge Language Models'>(Zhao et al, 2025b)</a> is actually higher than the figures report by <a href='https://arxiv.org/abs/2507.08838' title='wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models'>(Tang et al, 2025)</a>. -->

<p>◼️ <strong>IGPO</strong> <a href="https://arxiv.org/pdf/2509.10396" title="Inpainting-Guided Policy Optimization for DiffusionLarge Language Models">(Zhao et al, 2025b)</a>: shares the same first author as <strong>diffu-GRPO</strong>, and as mentioned above, uses the same likelihood estimation approach as <strong>diffu-GRPO</strong>. The novelty here is a procedure to leverage the inpainting capabilities inherent in DLMs (see my first <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P1#:~:text=strategies%20such%20as-,infilling,-.%20In%20the%20GIF">post</a>) to optimise the training efficiency and efficacy of GRPO. In GRPO,<label for="sidenote-refer" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-refer" class="margin-toggle" checked="" /><span class="sidenote">Refer to the image of PPO vs GRPO in the margins above for a sense.</span> when the entire group of sampled trajectories for a given prompt \(q_{k}\) (for e.g. a math problem) is zero, this results in no useful signal for the model to update its parameters. <strong>IGPO</strong>’s proposal assumes access to ground-truth or sufficiently high quality reasoning traces for \(q_{k}\) and to use some segment of the reasoning traces when such zero-reward groups are encountered. Specifically, by “seeding” a fragment of the reasoning trace amongst the masked tokens, we get a chance to steer the Masked-DLM towards generating a trajectory of good quality,<label for="sidenote-hint" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-hint" class="margin-toggle" checked="" /><span class="sidenote">It is akin to hinting to Jesse <em>“They have 2 plus 2 apples so that is 4 apples…“</em></span> which is then swopped with a zero-reward trajectory from the group. Training with this way to avoid zero-reward update steps led to stabler learning and enabled improvements on math and planning benchmarks over <strong>diffuGRPO</strong>; but importantly, it also outperforms <strong>uniGRPO</strong> that requires more samples to be taken for the likelihood estimation (per-trajectory per \(\mu\)-update vs per-prompt per \(\mu\)-update).<label for="sidenote-igporesults" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-igporesults" class="margin-toggle" checked="" /><span class="sidenote">See Table 1 of their paper <a href="https://arxiv.org/pdf/2509.10396" title="Inpainting-Guided Policy Optimization for DiffusionLarge Language Models">(Zhao et al, 2025b)</a>.</span></p>

<p>◼️ <strong>TraceRL</strong> <a href="https://arxiv.org/abs/2509.06949" title="Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models">(Wang et al, 2025)</a>: encapsulates some of the latest developments on a few fronts in Masked-DLM research. To summarise, they propose the use of a value model (another DLM) to manage the variance across updates (<em>à la</em> PPO). In addition, they leverage the semi-autoregressive approach of Fast-dLLM <a href="https://arxiv.org/abs/2505.22618" title="Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding">(Wu et al, 2025)</a>, which I discussed in my previous <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P3#:~:text=There%20are%20a-,number,-of%20approaches%20proposed">post</a>, that denoises blocks of tokens auto-regressively with efficiency via the use of approximated KV caches. This has the effect of giving a speed up to trajectory sampling, easing a major bottleneck especially for problems that are best solved with lengthy reasoning traces. To put these extensions together required special treatment (e.g. their §4.3), and this work is notable for putting forward a proposed solution for doing so. They report impressive performance on math benchmarks (87.4 for GSM8K and 94.2 for MATH500) that outperform the other methods listed above in this section,<label for="sidenote-tracerl" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-tracerl" class="margin-toggle" checked="" /><span class="sidenote">See Table 2 of their paper <a href="https://arxiv.org/abs/2509.06949" title="Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models">(Wang et al, 2025)</a>; and compare against results report in the other papers.</span> as well as ones for coding. Helpfully, the authors released the TraDo series of <a href="https://huggingface.co/collections/Gen-Verse/trado-series-68beb6cd6a26c27cde9fe3af">4B/8B parameters Masked-DLMs</a> that they trained with this approach, alongside their codebase.</p>

<h3 id="-4-whats-next">🤔 4. What’s next?</h3>
<p>To sum up, in this post, I started with a general overview of online RL post-training for LLMs. With some common ground established on that, I highlighted the main challenge for extending existing methods for AR-LLMs to Masked-DLMs, which is the issue of how to efficiently and accurately estimate token and sequence likelihoods needed for the PPO/GRPO objective. Finally, I gave an outline for recent work bringing PPO/GRPO to Masked-DLMs, focusing on how they proposed to address this estimation challenge.</p>

<p>This wraps up the first round on the topics I intended to cover in this series; the next posts – probably slightly further out – would look at all the areas I have discussed so far, but with multi-modality in consideration.</p>



    </article>
    <span class="print-footer">Diffusion Language Models -- Part Four (Post-training with Reinforcement Learning) - October 4, 2025 - Kelvin Han</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <!-- <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>     -->
    
      <li>
        <a href="//www.linkedin.com/in/kelvin-han"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="//www.x.com/kelvinhan"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//scholar.google.com/citations?user=AuOeIe8AAAAJ&hl=en"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//www.github.com/hankelvin"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2025 &nbsp;&nbsp;KELVIN HAN</span></br> <br>
<span>This site was created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte-Jekyll</a> theme in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
