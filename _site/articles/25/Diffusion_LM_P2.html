<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Diffusion Language Models -- Part Two (What kinds are there and how is one trained?)</title>
  <meta name="description" content="There are three variants of diffusion language models (DLMs), and the nuances of each impact their training, inference and scalability. I think it will be he...">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
<!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  


  <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/hankelvin.github.io/css/print.css" media="print"> -->

  <link rel="canonical" href="http://localhost:4000/hankelvin.github.io/articles/25/Diffusion_LM_P2">

  <link rel="alternate" type="application/rss+xml" title="Kelvin's blog" href="http://localhost:4000/hankelvin.github.io/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
	<a href="/hankelvin.github.io/"><img class="badge" src="/hankelvin.github.io/assets/img/L1016424.png" alt="CH"></a>
	
		
  	
		
		    
		      <a href="/hankelvin.github.io/">Posts</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/page/">Stuff</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/about/">About</a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/css/print.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/robots.txt"></a>
		    
	    
  	
		
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/latex.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/assets/css/main.css"></a>
		    
	    
  	
		
		    
		      <a href="/hankelvin.github.io/sitemap.xml"></a>
		    
	    
  	
	</nav>
</header>
    <article class="group">
      <h1 style="max-width: 50%; line-height: 1.5">Diffusion Language Models -- Part Two (What kinds are there and how is one trained?)</h1>
<p class="subtitle">August 1, 2025</p>

<p><span class="newthought">There are three variants of diffusion language models</span> (<strong>DLMs</strong>), and the nuances of each impact their training, inference and scalability. I think it will be helpful to situate them amongst each other before we proceed further; and so in this post, I will first introduce the variants, discuss their differences as well as what they mean. I will then go on to outline the training procedure for the Masked (<strong>Masked-DLM</strong>) variant as it is currently receiving significant amounts of attention in research<label for="sidenote-vogue" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-vogue" class="margin-toggle" checked="" /><span class="sidenote">And quite importantly, with useful extensions into multimodality and reinforcement learning already carried out with them.</span>, before ending off with a summary of two interesting pieces of DLM research<label for="sidenote-research" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-research" class="margin-toggle" checked="" /><span class="sidenote">
<br />‚óºÔ∏è (Wen et al, 2025) <a href="https://arxiv.org/abs/2507.11097">The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs</a> 
<br /> ‚óºÔ∏è (Prabhudesai et al, 2025) <a href="https://arxiv.org/abs/2507.15857">Diffusion Beats Autoregressive in Data-Constrained Settings</a></span> that surfaced recently along with thoughts on their broader implications. If you wish, you can use these links to skip to the specific sections of this post: <a href="#-1-what-variants-of-dlms-are-there">1. DLM variants</a>; <a href="#-2-apple-to-apple-whats-the-benefit-of-one-over-another">2. Comparing the variants</a>; <a href="#-3-what-is-the-training-procedure-for-a-dlm">3. Training a masked DLM</a>; and <a href="#-4-whats-come-up-recently-in-dlm-research">4. Recent findings and potential implications</a>
<!-- NOTE: "-" needed for space between emoji and first word --></p>

<h3 id="-1-what-variants-of-dlms-are-there">üé® 1. What variants of DLMs are there?</h3>
<p>DLM approaches can be described as being of the (i) <strong>Gaussian</strong>, (ii) <strong>Uniform</strong>, or (iii) <strong>Masked</strong> variants, based on how the original training data instances<label for="sidenote-datainstance" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-datainstance" class="margin-toggle" checked="" /><span class="sidenote">for e.g. an instance could be a sentence such as ‚Äúdogs are our friends‚Äù for the language modeling task.</span> are ‚Äúcorrupted‚Äù (or how the notion of noise is conceptualised and how it is introduced into the input during the foward process<label for="sidenote-reverse" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-reverse" class="margin-toggle" checked="" /><span class="sidenote">. . . and hence (by design) the reverse process as well.</span>). The first two are named for the distributions they use to draw noise from, and the third is likely named for a special token (e.g. <code class="language-plaintext highlighter-rouge">[MASK]</code>) added to the vocabulary to give a noised state that ‚Äúmasks‚Äù the original token. To give a more concrete feel of each variant, I will use the following toy example to illustrate some of them.</p>

<div style="background-color: #249ae9ff; max-width: 50%; color: white; padding: 20px; border-radius: 8px; margin: 10px;">
  <h3 style="margin: 0 0 15px 0; width: 100%;">Settings for a toy example</h3>   
  <p style="margin: 0; width: 100%; text-align: justify;">   
    Imagine we have a toy language with 13 lexical units in the vocabulary V = <em>{"we", "you", "they", "our", "your", "their", "cat", "dog", "friend", "is", "are", "s", "[PAD]"}</em>, which allows one to form sequences such as <em>"dog s are our friend s"</em>, <em>"our dog s are your s"</em>, <em>"your cat is our friend"</em> etc. 
    <br /><br />
    <small><small>The special marker [PAD] is used as a filler to ensure that all sequences are of the same length, e.g. "<em>your cat is our friend [PAD]</em>", so that it has the same six-unit length as the other two dog-sentences, allowing us to do batched generations.</small></small>
  </p>
</div>

<p>‚óºÔ∏è <strong>Gaussian-DLM</strong>: This variant, also referred to as ‚Äúcontinuous-time‚Äù in the literature, is the closest in form to the ones that are found in image diffusion models<label for="sidenote-imagediff" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-imagediff" class="margin-toggle" checked="" /><span class="sidenote">Such as Google‚Äôs <a href="https://deepmind.google/models/imagen/">Imagen</a>, OpenAI‚Äôs <a href="https://openai.com/index/dall-e-3/">DALL-E</a> and Stability AI‚Äôs <a href="https://stability.ai/stable-image">Stable Diffusion</a></span>, so if you are familiar with them, then the Gaussian-DLM models could be quite familiar to you. The architecture here involves: (i) embedding the tokens of a sequence so that each of them are represented with real-valued vectors<label for="sidenote-embed" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-embed" class="margin-toggle" checked="" /><span class="sidenote">This is similar to the first step in auto-regressive LLMs (<strong>AR-LLMs</strong>) (for Transformers as well as RNNs).</span>, and (ii) adding to them noise that is in the form of random vectors drawn from a Gaussian distribution \(\epsilon_{t}\) ~ \(N(\mu_{t}, \sigma_{t}^2)\) at each step. Using the toy example, what this means in practice is that we will keep a look-up table for 13 random vectors (one for every unit in our toy language‚Äôs vocabulary) and each vector is of a dimension \(d\). So the sequence <em>‚Äúdog s are our friend s‚Äù</em> will be represented by six token vectors, and we will add noise to each token vector such that every token is completely gaussian at the limit of the forward process i.e. \(t \to \infty\) (or in practice, some defined terminal timestep \(T\) so that the total noise added is known; it is usually set at 1.0 in diffusion models). A few formulations have been proposed for learning these models, including score-matching which appears most commonly, as well as latent variable models and stochastic differential equations, for learning the reverse process <a href="https://arxiv.org/abs/2211.15089" title="Continuous diffusion for categorical data">(Dieleman et al, 2023)</a>. Examples of Gaussian-DLMs include (i) Diffusion-LM <a href="https://arxiv.org/abs/2205.14217" title="Diffusion-LM Improves Controllable Text Generation">(Li et al, 2022)</a> that appeared in 2022 and which first drew attention towards diffusion modeling for text generation, (ii) CDCD <a href="https://arxiv.org/abs/2211.15089" title="Continuous diffusion for categorical data">(Dieleman et al, 2023)</a>, and (iii) PLAID <a href="https://arxiv.org/abs/2305.18619" title="Likelihood-Based Diffusion Language Models">(Gulrajani &amp; Hashimoto, 2023)</a>.</p>

<!-- <label for='sidenote-early-discrete' class='margin-toggle sidenote-number'></label><input type='checkbox' id='sidenote-early-discrete' class='margin-toggle' checked/><span class='sidenote'>Earlier models examined diffusion for discrete sequences such as music <a href='https://arxiv.org/abs/2103.16091' title='Symbolic Music Generation with Diffusion Models'>(Mittal et al, 2021)</a>, but not specifically on language and at the same scale of a language model. Initial explorations can be found in some of a few earlier discrete diffusion work such as <a href='https://arxiv.org/abs/2107.03006' title='Structured Denoising Diffusion Models in Discrete State-Spaces'>(Austin et al, 2021)</a> -- see ¬ß3.1 there</span> -->

<p>‚óºÔ∏è <strong>Uniform-DLM</strong>: Instead of the real-valued embedding vectors used in Gaussian-DLMs, this approach represents each token of a sequence with a one-hot vector<label for="sidenote-onehot" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-onehot" class="margin-toggle" checked="" /><span class="sidenote">i.e. the vector for each token is a dirac distribution with all probability mass concentrated at the actual token‚Äôs index in the vocabulary.</span>; of dimensions the size of the vocabulary; whereby it is 1 at the position of the token in the vocabulary, and 0 elsewhere. Here, the notion of adding noise to the data instance<label for="sidenote-udlmnoise" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-udlmnoise" class="margin-toggle" checked="" /><span class="sidenote">Note that noise is added/removed independently between each token in the sequence ‚Äì ‚Äú<em>We make the assumption that the forward noising process is applied independently across a sequence‚Ä¶ the denoising process factorizes independently across tokens.</em>‚Äù <a href="https://arxiv.org/abs/2406.07524" title="Simple and Effective Masked Diffusion Language Models">(Sahoo et al, 2024)</a> </span> is via the application of some transition matrix (\(Q_t\), determining the probabilities for whether the token stays unchanged or to which one of the other vocab units) such that the initially concentrated probability mass in the token vector gradually distributes over all of the other vocab units. At the limit of the forward process, the one-hot vector applied with \(Q_{t=T}\) would give a uniform distribution (i.e. each vocab unit is equally likely; there is completely no useful signal to deduce what the original token was) and also reach stationarity, i.e. additional steps cannot change from the uniform distribution. Examples of this approach include the <code class="language-plaintext highlighter-rouge">Uniform</code> versions of the models trained in <a href="https://arxiv.org/abs/2310.16834" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">(Lou et al, 2023)</a> and <a href="https://arxiv.org/abs/2310.16834" title="Simple Guidance Mechanisms for Discrete Diffusion Models">(Schiff et al, 2025)</a>.<label for="sidenote-wheel-udlm" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-wheel-udlm" class="margin-toggle" checked="" /><span class="sidenote">To extend the <strong>Wheel</strong> analogy to Uniform-DLM: (i) instead of each panel on the gameboard being two-sided (being either white/blank or a character), they would be |\(V\)|-sided (i.e. as many sides as the vocabulary and without white/blank), (ii) the gameboard will start with some completely scrambled combination of characters, and (iii) at every guess, the contestant can flip multiple panels to any other character in the vocabulary. Another way to look at it could be as a slots machine (see GIF below).</span></p>

<p><label for="marginfigure-slots" class="margin-toggle">‚äï</label><input type="checkbox" id="marginfigure-slots" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/udlm.gif" /><br />Source: Slots GIF ‚Äì <a href="https://discrete-diffusion-guidance.github.io/">https://discrete-diffusion-guidance.github.io/</a></span></p>

<p>‚óºÔ∏è <strong>Masked-DLM</strong>: This is also referred to as modeling discrete diffusion with an ‚Äúabsorbing state‚Äù, first appearing in <a href="https://arxiv.org/abs/2107.03006" title="Structured denoising diffusion models in discrete state-spaces">(Austin et al, 2021)</a>. Essentially, noise is represented by the special token mentioned above<label for="sidenote-maskexp" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-maskexp" class="margin-toggle" checked="" /><span class="sidenote"> for e.g., the sentence <em>‚Äúdog s are our friend s‚Äù</em> could be gradually masked to ‚Äúdog [MASK] are our [MASK] s‚Äù and finally to ‚Äú[MASK] [MASK] [MASK] [MASK] [MASK] [MASK]‚Äù </span>, i.e. a token in the original sequence is corrupted by being ‚Äúabsorbed‚Äù into this special state (rather than transitioning to others). This is an important characteristic of most current Masked-DLMs, i.e. in the forward process, once a token transitions to <code class="language-plaintext highlighter-rouge">[MASK]</code>, it stays in that state throughout the subsequent steps. Conversely, in the reverse process, once a <code class="language-plaintext highlighter-rouge">[MASK]</code> token transitions to a vocab unit \(v\) other than <code class="language-plaintext highlighter-rouge">[MASK]</code>, it also stays as \(v\) in all subsequent steps.<label for="sidenote-wheelmdlm" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-wheelmdlm" class="margin-toggle" checked="" /><span class="sidenote">This variant matches the <strong>Wheel</strong> analogy in the previous <a href="https://hankelvin.github.io/articles/25/Diffusion_LM_P1">post</a>; the white panels correspond to the <code class="language-plaintext highlighter-rouge">[MASK]</code> token, behind each white panel is one character and once a contestant makes a <del>correct</del> any guess for it, it cannot be changed.</span> Masked-DLM is the basis for the LLaDA <a href="https://arxiv.org/abs/2502.09992" title="Large Language Diffusion Models">(Nie et al, 2024)</a> and Dream <a href="https://hkunlp.github.io/blog/2025/dream" title="Dream 7B"> (Ye et al, 2025)</a> models, as well as well as multimodal versions such as LLaDA-V <a href="https://arxiv.org/abs/2505.16933" title="LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning">(You et al, 2025)</a> and MMaDA <a href="https://arxiv.org/abs/2505.15809" title="MMaDA: Multimodal Large Diffusion Language Models">(Yang et al, 2025)</a>, It was also explored in SEDD <a href="https://arxiv.org/abs/2310.16834" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">(Lou et al, 2023)</a>, which Inception Lab‚Äôs Mercury models are reportedly based on<label for="sidenote-mercury" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-mercury" class="margin-toggle" checked="" /><span class="sidenote">The Mercury technical report <a href="https://arxiv.org/abs/2506.17298" title="Mercury: Ultra-Fast Language Models Based on Diffusion">(Inception Labs, 2025)</a> state that ‚Äú<em>Our methods extend <a href="https://arxiv.org/abs/2310.16834" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">(Lou et al, 2023)</a> through careful modifications to the data and computation to scale up learning.</em>‚Äù. Although they do not state which of the Masked-DLM or Uniform-DLM variant they studied in (Lou et al, 2023) they ended up leveraging, it seems possible that the Gaussian-DLM approach was taken given the poorer perplexity figures obtained by Uniform-DLM in their work as well as in earlier work.</span>.</p>

<p><ins><strong>Light round-up:</strong></ins> At the limit \(T\) in their forward processes, each class of DLM can be summarised as follows: for Gaussian-DLMs each token in a sequence can transition to any other token (state) reachable by the accumulated variance of the sampled noise; for Uniform-DLMs, each token can transition to any other state with equal probability; and in the case of Masked-DLMs, each token lands on the special <code class="language-plaintext highlighter-rouge">[MASK]</code> token.</p>

<p><ins><strong>Masked-DLM connections with BERT:</strong></ins> Recall that BERT <a href="https://arxiv.org/abs/1810.04805" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding">(Devlin et al, 2018)</a> ‚Äì an encoder-only model that could be used for tasks such as cloze-style QA<label for="sidenote-cloze" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-cloze" class="margin-toggle" checked="" /><span class="sidenote">e.g the most likely token after <em>‚ÄúParis is the capital of‚Äù</em> is <em>‚ÄúFrance‚Äù</em>. </span> and which is a precursor to the LLMs of today ‚Äì was trained with a masked language modeling objective, i.e. random portions (~15%) of the sentences in the training data are masked, and the model has to learn to predict the masked words. This is very similar to the Masked-DLM approach, except that in Masked-DLMs this unmasking is done across multiple steps (instead of a single pass), and for the entire sequence (the last Masked-DLM inference step would most closely align with the task in the BERT MLM objective). Accordingly, some work have explored leveraging BERT-style (i.e. encoder-only) models for DLM: see DiffusionBERT <a href="https://aclanthology.org/2023.acl-long.248" title="DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models">(He et al, 2023)</a> which propose further training BERT with time-embeddings with a special diffusion noise schedule to use it <em>√† la DLM</em>. See also <em>‚ÄòComparison to BERT‚Äô</em> in ¬ß6 of <a href="https://arxiv.org/abs/2406.07524" title="Simple and Effective Masked Diffusion Language Models">(Sahoo et al, 2024)</a> for a discussion on this connection.</p>

<h3 id="-2-apple-to-apple-whats-the-benefit-of-one-over-another">üçè 2. Apple-to-apple: what‚Äôs the benefit of one over another?</h3>
<p>There is more focus on Masked-DLM and Uniform-DLM over Gaussian-DLM currently, as the latter has been met with comparably less success (in terms of achievable perplexity).<label for="sidenote-clamp" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-clamp" class="margin-toggle" checked="" /><span class="sidenote">The reason for this is because the question of how to reconcile the noised real-valued vectors in Gaussian-DLM to discrete space is not trivial and required many special tricks for training and inference to work. For instance, it was necessary to use nearest neighbour search and clamping to a valid token vector at every reverse diffusion step in Diffusion-LM <a href="https://arxiv.org/abs/2205.14217" title="Diffusion-LM Improves Controllable Text Generation">(Li et al, 2022)</a> to reach comparable performance with AR-LLMs.</span> In the most recent round of published DLM research (i.e. ICLR and ICML in 2025), significant attention has been focused on Masked-DLM approaches<label for="sidenote-duo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-duo" class="margin-toggle" checked="" /><span class="sidenote">Though it should be noted that interesting work on Gaussian-DLM &amp; Uniform-DLM is also continuing, I think notably in <a href="https://arxiv.org/abs/2506.10892" title="The Diffusion Duality">(Sahoo et al, 2025)</a>, where they work out a proof that connects Uniform-DLM as a special case of Gaussian-DLM using the argmax operator, and opens up the possibility to leverage techniques in Gaussian-DLM for training and inferencing on Uniform-DLM.</span>, as they achieve better perplexity compared to Uniform-DLM<label for="sidenote-perplexity" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-perplexity" class="margin-toggle" checked="" /><span class="sidenote">Note that lower is better for perplexity; compare the SEDD (Uniform) and SEDD (Absorb) results in Table 1 of <a href="https://arxiv.org/abs/2310.16834v2" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">(Lou et al, 2025)</a>; as well as D3PM uniform vs D3PM absorbing in Figure 2 and Table 2 of <a href="https://arxiv.org/abs/2107.03006" title="Structured Denoising Diffusion Models in Discrete State-Spaces">(Austin et al, 2021)</a>.</span>. Intuitively, this is not unexpected, as it would seem easier to learn an Masked-DLM (where transitions are constrained once the absorbing <code class="language-plaintext highlighter-rouge">[MASK]</code> state is reached in the forward process) compared to Uniform-DLM (where transitions at each timestep could be to any other tokens, i.e. a much larger space of posssible transitions).</p>

<!-- - "such as efficient sampling algorithms based on advanced ODE solvers, or classifier-free guidance." (Dieleman et al, 2023) -->
<p>However, several shortcomings of the Masked-DLM approach have been raised (hence the continued research interests in Gaussian-DLM and Uniform-DLM approaches). Chief amongst them include: (i) the potentially over-restrictiveness of the vanilla Masked-DLM masking/unmasking procedure<label for="sidenote-vanillamaskinference" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-vanillamaskinference" class="margin-toggle" checked="" /><span class="sidenote">I use ‚Äúvanilla‚Äù because there are approaches emerging that propose more advanced inference procedures such as remasking that could address this. For instance <a href="https://arxiv.org/abs/2503.00307" title="Remasking discrete diffusion models with inference-time scaling">(Wang et al, 2025)</a></span>; and (ii) the difficulty of introducing classifier-free guidance into Masked-DLMs.</p>

<p>‚óºÔ∏è regarding shortcoming (i): ‚Äú<strong>Self-correction</strong>‚Äù is a term for referring to how tokens can continue to transition to others over the reverse diffusion steps; it is connected to the ‚Äúcoarse-to-fine‚Äù property (in the truest sense) that DLMs are frequently touted to beneficially possess over AR-LLMs. While self-correction is inherently possible in Gaussian-DLMs and Uniform-DLMs given their design<label for="sidenote-predictor" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-predictor" class="margin-toggle" checked="" /><span class="sidenote"><ins><em>Sidenote:</em></ins> they could also be steered with predictor-corrector models like in image diffusion models <a href="https://openreview.net/forum?id=VM8batVBWvg" title="Discrete Predictor-Corrector Diffusion Models for Image Synthesis">(Lezama et al, 2023)</a></span>, it is not possible with Masked-DLM (without some engineering). Theoretically, this lack of self-correction could give rise to errors in the inference steps, which would then propagate<label for="sidenote-selfcorrect" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-selfcorrect" class="margin-toggle" checked="" /><span class="sidenote">To give a concrete example, say we start from ‚Äú[MASK] [MASK] [MASK] [MASK] [MASK] [MASK]‚Äù, errors (e.g. in modeling learning) could bring us to ‚Äú[MASK] [MASK] are your s [PAD]‚Äù and since unmasked tokens cannot transition any further, this leaves limited choice but to go to ‚Äúour dog are your s [PAD]‚Äù resulting in a number agreement error. <br /><ins><em>Sidenote:</em></ins> It could be interesting to carry out a study at-scale of the tokens (and the types of words they form) that get unmasked across the Masked-DLM inference steps ‚Äì e.g. see Figures 10-15 of <a href="https://arxiv.org/abs/2107.03006" title="Structured Denoising Diffusion Models in Discrete State-Spaces">(Austin et al, 2023)</a>, and establish whether there are any significant patterns in what parts of a sentence/paragraph gets unmasked and fixed first.</span>. That said, some recent studies proposed remasking strategies to address such issues ‚Äì e.g. <a href="https://arxiv.org/abs/2407.21243" title="Informed Correctors for Discrete Diffusion Models">(Zhao et al, 2025)</a>, <a href="https://arxiv.org/abs/2502.09992" title="Large Language Diffusion Models">(Nie et al, 2024)</a>. <a href="https://arxiv.org/pdf/2503.00307v1" title="Remasking Discrete Diffusion Models with Inference-Time Scaling">(Wang et al, 2025)</a> also claim to provably show the soundness of applying remasking without needing to take special considerations into the training and inference of Masked-DLMs. In practice, the LLaDA authors <a href="https://arxiv.org/pdf/2502.09992" title="Large Language Diffusion Models"> (Nie et al, 2025)</a> were able to reach AR-LLM performance for their model whilst leveraging generation with remasking strategies (see ¬ß2.4 of their paper).</p>

<p>‚óºÔ∏è regarding shortcoming (ii): <strong>Guidance</strong> originates from diffusion modeling for image and refers to conditioning information (for instance a label such as ‚Äúdogs‚Äù, or a text prompt such as ‚Äúfriendly-looking dogs‚Äù) we can add to ‚Äúguide‚Äù the reverse diffusion process towards generating an image with certain desired properties.<label for="sidenote-cfg" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-cfg" class="margin-toggle" checked="" /><span class="sidenote">This was initially proposed with the use of gradients from a classifier <a href="https://arxiv.org/abs/2105.05233" title="Diffusion models beat GANs on image synthesis">(Dhariwal &amp; Nichol, 2021)</a> that can identify the classes of the images during training; but since training is over a diffusion process, the classifier had to be trained to be able to identify the classes across the noising process, which can be complicated to achieve. <a href="https://arxiv.org/abs/2207.12598" title="Classifier-Free Diffusion Guidance">(Ho &amp; Salimans, 2022)</a> established a more efficient and effective to train an image diffusion model for guidance without the need for a separate classifier (classifier free guidance, or <strong>CFG</strong>) which is now widely used. See this Sander Dieleman <a href="(https://sander.ai/2022/05/26/guidance.html)">post</a> for an overview. For a more visual explanation of CFG (and also diffusion models in general), have a look at this recent <a href="https://youtu.be/iv-5mZ_9CPY?t=1791">Welch Labs-3Blue1Brown explainer</a>.</span> Guidance is useful for DLM too, as a way to steer towards safety and better matching user intent, for e.g. to adhere to style requirements or reflect concepts such as non-toxicity, inclusivity, empathy, neutrality etc. An example of such work is DGLM <a href="https://aclanthology.org/2024.findings-acl.887" title="Diffusion Guided Language Modeling">(Lovelace et al, 2024)</a> which uses a Gaussian-DLM to produce a ‚Äúcandidate‚Äù continuation of a prompt plus some guidance condition, which<label for="sidenote-cand" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-cand" class="margin-toggle" checked="" /><span class="sidenote">The input to the AR-LLM is the probability distribution of the candidate denoised/‚Äùrefined‚Äù by the Gaussian-DLM from noise.</span> is then put through a decoder-only AR-LLM to verbalise.<label for="sidenote-weinberger" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-weinberger" class="margin-toggle" checked="" /><span class="sidenote">For a quick overview: see Kilian Weinberger‚Äôs <a href="https://youtu.be/klW65MWJ1PY?t=2757">presentation</a> on the work.</span> However, standard classifier-free guidance (<strong>CFG</strong>) are designed with diffusion models trained with the score-matching objective (which learns a model to match the <em>score</em>, or the gradient of the log probability density function with respect to the data) in mind. Score-matching in the continuous sense is not used for Masked-DLMs<label for="sidenote-discretescore" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-discretescore" class="margin-toggle" checked="" /><span class="sidenote">Though there is concrete score matching <a href="https://arxiv.org/pdf/2211.00802" title="Concrete Score Matching: Generalized Score Matching for Discrete Data">(Meng et al, 2023)</a> for the discrete case (modified in SEDD <a href="https://arxiv.org/abs/2310.16834" title="Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution">(Lou et al, 2023)</a>), it is an approximation and does not address how classical guidance (as in the continuous setting for images) can be applied (without potentially scrambling the discrete semantics). Moreover, recent Masked-DLM work such as in the LLaDA family of models directly set the training objective as the cross-entropy loss on the masked tokens (Eq 3 in <a href="https://arxiv.org/pdf/2502.09992" title="Large Language Diffusion Models">(Nie et al, 2025)</a>), a further departure from the score-matching formulation for learning DLMs.</span> hence it is not feasible to transfer existing CFG techniques to Masked-DLMs. Notably however, <a href="https://arxiv.org/abs/2410.18514" title="Scaling up Masked Diffusion Models on Text">(Nie at al, 2024)</a> proposed <em>unsupervised classifier-free guidance</em>, a training objective to allow CFG in Masked-DLMs without using paired data (e.g. prompt-continuation, question-answer); they found that unsupervised CFG fine-tuning of an Masked-DLM gives better performance compared to using standard CFG.</p>

<h3 id="Ô∏èÔ∏è-3-what-is-the-training-procedure-for-a-dlm">üèãÔ∏è‚Äç‚ôÄÔ∏è 3. What is the training procedure for a DLM?</h3>
<p>To get a general sense of the DLM training procedure, it will be useful to look at the LLaDA approach as it includes language model pretraining and instructions fine-tuning, both of which are fundamental for general-purpose LLM usage. Note that most work parameterise their DLMs using the Transformer architecture<label for="sidenote-transformer" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-transformer" class="margin-toggle" checked="" /><span class="sidenote">Paralleling the trend in image diffusion too <a href="https://arxiv.org/pdf/2202.04200" title="MaskGIT: Masked Generative Image Transformer">(Chang et al, 2022)</a> as well as <a href="https://arxiv.org/pdf/2212.09748" title="https://arxiv.org/pdf/2212.09748">(Peebles &amp; Xie, 2023)</a></span>, but this is not a must.<label for="sidenote-rush" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-rush" class="margin-toggle" checked="" /><span class="sidenote">Alexander Rush has a useful <a href="https://www.youtube.com/watch?v=WjAUX23vgfg">explainer video</a> of diffusion models for text in general.</span> Here I will briefly touch on two key areas of the training procedure: (i) the noising in the forward process, and (ii) the training objective. For more details and code on LLaDA‚Äôs training, see their <a href="https://ml-gsai.github.io/LLaDA-demo/">blogpost</a> and <a href="https://github.com/ML-GSAI/LLaDA/blob/main/GUIDELINES.md">codebase</a>.</p>

<p><br /></p>

<figure><figcaption><span>Image source: illustrating the masking and prediction procedure for Masked-DLM pretraining and fine-tuning ‚Äì <a href="https://arxiv.org/pdf/2502.09992" title="Large Language Diffusion Models"> (Nie et al, 2025)</a> <br /><br /></span></figcaption><img src="/hankelvin.github.io/assets/img/llada_training.png" /></figure>

<p>‚óºÔ∏è <strong>Noising:</strong> The general idea is to sample some noise level for a given data instance (see graphic above for how masking is done for pretraining and for instructions fine-tuning) which will be used to determine the tokens to mask (e.g. in footnote 10 above on the sequence ‚Äúdog s are our friend s‚Äù) in the forward process. The noising schedule (e.g. linear, geometric or even cosine) can impact performance; for instance, if we model it such that masking is at a slower rate as \(t \to T\) in the forward process, then this would align with an unmasking sequence in the reverse denosing process where we unmask comparatively fewer tokens at first before gradually increasing.<label for="sidenote-schedule" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-schedule" class="margin-toggle" checked="" /><span class="sidenote">Intuitively, this makes sense since tokens are unmasked independently of each other at each time step, and given the fixedness in Masked-DLMs unmasking, we want fewer commitments at the start to reduce the unmasking of conflicting tokens, which will go on to compound in subsequent steps.</span> On another note, multiple epochs (together with full-attention over the sequence length at every step drive compute for DLM training up; &lt;=64x, see <a href="https://arxiv.org/pdf/2305.18619" title="Likelihood-Based Diffusion Language Models">(Gulrajani &amp; Hashimoto, 2023)</a>) of training over the data are needed in practice, with (almost likely) different noise sampled on the same data instance, is required for DLMs to reach the same level of performance on perplexity as AR-LLMs (which are typically trained with a single epoch over the data); while less efficient to train, it is this procedure that imbues the DLM to be able to generate non-autoregressively.</p>

<p>‚óºÔ∏è <strong>Training objective:</strong> In the earliest diffusion models, the training objective involves having to compute the evidence lower bound (ELBO)<label for="sidenote-elbo" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-elbo" class="margin-toggle" checked="" /><span class="sidenote">Unlike AR-LLMs which factorise the likelihood of a sequence into conditional probabilities (i.e. probability over the vocabulary at every step) which make it easier to evaluate, likelihood of a sequence (which is how we model in DLM) is intractable, hence the use of ELBO.</span>; subsequently, a simplification to the original ELBO was proposed with the score-matching approach<label for="sidenote-score" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-score" class="margin-toggle" checked="" /><span class="sidenote"><a href="https://arxiv.org/pdf/2006.11239" title="Denoising Diffusion Probabilistic Models">(Ho et al, 2020)</a> showed that the original ELBO in the diffusion objective can be further simplified by modeling the probability distributions as <em>scores</em> (gradient of the log prob with respect to the data), and the objective can be reduced to minimising the mean-squared error of the entries in the predicted score vector and the true score, which is substantially easier to do.</span>. Since the noise level varies across time, it was also included in a time variable (for e.g. as an additional embedding in CDCD). Recently, it was shown (quite concurrently, based on version dates on arxiv) in MD4 <a href="https://arxiv.org/abs/2406.04329v4" title="Simplified and Generalized Masked Diffusion for Discrete Data">(Shi et al, 2024)</a>, MDLM <a href="https://arxiv.org/pdf/2406.07524" title="Simple and Effective Masked Diffusion Language Models">(Sahoo et al, 2024)</a> and RADD <a href="https://arxiv.org/abs/2406.03736" title="Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data">(Ou et al, 2024)</a> that, for Masked-DLMs, a tighter bound could be obtained by modeling the predictions of the logits at each timestep against the ground truth (‚Äú<em>clean data</em>‚Äù) labels using cross-entropy, and that it is fine to drop the need to explicitly capture the time variable, which greatly simplifies the training objective (and becomes very similar to the standard AR-LLM training objective). Notably, the use of this cross-entropy objective is validated empirically by the strong evaluations from the LLaDA model, whose training was done with it.</p>

<h3 id="-4-whats-come-up-recently-in-dlm-research">üì∞ 4. What‚Äôs come up recently in DLM research?</h3>
<p>On another note, two interesting pieces of research have surfaced last week which I think adds to the conversation about DLMs.</p>

<p>‚óºÔ∏è The first is from <a href="https://arxiv.org/abs/2507.11097" title="The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs"> (Wen et al, 2025)</a> which studies the jailbreaking vulnerability of text and multimodal Masked-DLMs (LLaDA, Dream and MMaDA). They find that these Masked-DLMs (see Appendix C of their paper) ‚Äú<em>often match or surpass those of autoregressive LLMs in resisting existing jailbreak attack methods</em>‚Äù, which is ideal. However, they also found that it is possible to use AR-LLMs (GPT4o or a 7B-parameter Qwen model) few-shot prompting to generate ‚Äúmid-flight‚Äù unmasked sequences (i.e. \(\hat{x}_{t}, 0 &lt; t &lt; T\)) which an Masked-DLM would go on to unmask jailbroken content (see example on right column). Notably, they tested their method on several jailbreaking benchmarks and established similar findings of such behaviour across Masked-DLMs, including going from \(\leq\) 1% success on JailbreakBench to \(\geq\) 99% success. They also found that simply extending the length of the sequence made it possible to bring the Masked-DLM from initial refusal to a jailbreak outcome (see next example on right column). These findings flag the need for further efforts to study DLM jailbreaking, to better understand and find ways to mitigate such safety weaknesses in them.</p>

<p><label for="marginfigure-jailbreak" class="margin-toggle">‚äï</label><input type="checkbox" id="marginfigure-jailbreak" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/jailbreak_llada.png" /><br />Source: jail breaking example ‚Äì <a href="https://arxiv.org/abs/2507.11097" title="The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs"> (Wen et al, 2025)</a></span></p>

<p><label for="marginfigure-jailbreak-longer" class="margin-toggle">‚äï</label><input type="checkbox" id="marginfigure-jailbreak-longer" class="margin-toggle" checked="" /><span class="marginnote"><img class="fullwidth" src="/hankelvin.github.io/assets/img/jailbreak_longergen.png" /><br />Source: jail breaking example ‚Äì <a href="https://arxiv.org/abs/2507.11097" title="The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs">(Wen et al, 2025)</a>. <em>This finding indicates some conditional dependence for generating refusal content which is tied to max sequence length seen at training, i.e. this might be resolvable by taking steps to disconnect this dependence during training.</em></span></p>

<p>‚óºÔ∏è The other work <a href="https://arxiv.org/abs/2507.15857v1" title="Diffusion Beats Autoregressive in Data-Constrained Settings">(Prabhudesai et al, 2025)</a> studies the compute-data Pareto frontier of DLMs and 100 comparable AR-LLMs. Specifically, they trained 100 Masked-DLMs and AR-LLMs (ranging in size from 7M- to 2.5B-parameters) on the English C4 corpus (at data scales of between 25 to 100M tokens) for up to 800 epochs. From their study, they found that initially, at low epoch counts, AR-LLMs outperforms DLMs; but as repeated passes over the data is carried out, DLMs overtake and perform better. Their experiments allowed them to establish a potential scaling law for for DLMs, and also conclude that under data-constrained settings (for e.g. when Internet data peters out, or in sequence modeling for specialised domains/applications where available data could be at smaller scales), a DLM architecture may be better for modeling the data distribution. The findings provide useful insights that help clarify whether (and where) moving to DLMs makes sense. That said, we are still lacking an understanding of the DLM &amp; AR-LLM differences under real-use case evaluations<label for="sidenote-eval" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sidenote-eval" class="margin-toggle" checked="" /><span class="sidenote">e.g. with suites such as <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>; in their work they only report loss curves and perplexity (NLL).</span> as well as the impact of methods that have been recent drivers of AR-LLM model improvements (such as training on synthetic data, as well as preference tuning and reinforcement learning) on DLMs.</p>

<p><label for="marginnote-resources" class="margin-toggle"> ‚äï</label><input type="checkbox" id="marginnote-resources" class="margin-toggle" checked="" /><span class="marginnote">Some useful resources for diffusion modeling: Firstly, the ‚ÄúDiffusion Models‚Äù chapters in: 
<br />‚óºÔ∏è Chapter 20 of <a href="https://www.bishopbook.com/">Deep Learning: Foundations and Concepts</a>. Bishop, C.M., Bishop, H. (2024). Springer.; 
<br />‚óºÔ∏è Chapter 25 of <a href="https://probml.github.io/pml-book/book2.html">Probabilistic Machine Learning: Advanced Topics</a>. Murphy K. P. (2023). MIT Press.; 
<br />‚óºÔ∏è Chapter 18 of <a href="https://udlbook.github.io/udlbook/">Understanding Deep Learning</a>. Prince, S. J. D. (2024). MIT Press. 
<br /><em>Of the three textbooks, the Murphy one (to my mind, the bible of probabilistic generative modeling for its breadth and depth) has the most substantial coverage of discrete diffusion modeling, and the Bishop one is (in my opinion) the most accessibly written plus it also comes with helpful sections on score matching and guidance; though it helped to triangulate information between the three as much as possible.</em>
<br />Secondly, it also helps to start with the score-matching diffusion models that were developed for image generation and go on to discrete case. To help: the diffusion and flow modules of the CS236 class taught by Stefano Ermon could be very useful for putting the parts on together.
<br />‚óºÔ∏è <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8">Stanford CS236: Deep Generative Models 2023 playlist</a>
Finally, this survey paper also goes deep into the various aspects of DLMs:
<br />‚óºÔ∏è <a href="https://arxiv.org/abs/2506.13759?">Discrete Diffusion in Large Language and Multimodal Models: A Survey</a></span></p>

<p><label for="marginnote-notes" class="margin-toggle"> ‚äï</label><input type="checkbox" id="marginnote-notes" class="margin-toggle" checked="" /><span class="marginnote"><ins><strong>Some notes:</strong></ins> It seems reasonable to consider combining Uniform-DLM and Masked-DLM, after all the latter is just adding an additional state <code class="language-plaintext highlighter-rouge">[MASK]</code> and enforcing fixedness once this state is reached/exited. I am wondering if it might make sense to add the <code class="language-plaintext highlighter-rouge">[MASK]</code> token as well as permit transitions to and from it across the timesteps (i.e. Uniform-DLM+<code class="language-plaintext highlighter-rouge">[MASK]</code>), but with some constraints that the overall ratio of <code class="language-plaintext highlighter-rouge">[MASK]</code> tokens must monotonically increase over time in the forward process, which could address the ‚Äúself-correction‚Äù limitation of Masked-DLM (see <a href="#-2-apple-to-apple-whats-the-benefit-of-one-over-another">above</a>). <a href="https://arxiv.org/abs/2107.03006" title="Structured Denoising Diffusion Models in Discrete State-Spaces">(Austin et al, 2021)</a> (see Appendix A.2.6 and B.2.1 as well as Figure 4 (upper) there) stated that they carried out some ablations on the text8 dataset that touched on this ‚Äì for e.g. by applying \(e_m\) a separate one-hot vector with 1 on <code class="language-plaintext highlighter-rouge">[MASK]</code> and 0 elsewhere ‚Äì but do not seem to have included the results to show how Uniform-DLM+<code class="language-plaintext highlighter-rouge">[MASK]</code> performs over Masked-DLM. <em>Although this change might impact the simplification of the loss objective to the use of cross-entropy against ground truth tokens as established by MD4, MDLM and RADD. (see for e.g. ¬ß3.1 of the RADD paper)</em></span></p>

<h3 id="-4-whats-next">üëâ 4. What‚Äôs next?</h3>
<p>In this post, I have covered the three broad classes of DLMs, discussed what their differences mean, then briefly described the training procedure of Masked-DLMs, before ending on some recent DLM research and their broader implications. In the next post, I will walk through the reverse process used for generation in DLMs, situate generation with DLMs against existing efficient serving methods for AR-LLMs, and then look at the a few recent advanced sampling techniques proposed (such as block-wise/semi-autoregessivity and caching). In the post after that, I will cover preference tuning and reinforcement learning of these DLMs.</p>

<p><em>Update (16 August 2025): (1) precise DLM‚Äôs 64x more FLOPs statement, (2) refine <em>Training objective</em> subsection.</em></p>



    </article>
    <span class="print-footer">Diffusion Language Models -- Part Two (What kinds are there and how is one trained?) - August 1, 2025 - Kelvin Han</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <!-- <li><a href="mailto:hate@spam.net"><span class="icon-mail3"></span></a></li>     -->
    
      <li>
        <a href="//www.linkedin.com/in/kelvin-han"><span class="icon-linkedin"></span></a>
      </li>
    
      <li>
        <a href="//www.x.com/kelvinhan"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//scholar.google.com/citations?user=AuOeIe8AAAAJ&hl=en"><span class="icon-google2"></span></a>
      </li>
    
      <li>
        <a href="//www.github.com/hankelvin"><span class="icon-github"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2025 &nbsp;&nbsp;KELVIN HAN</span></br> <br>
<span>This site was created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte-Jekyll</a> theme in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
