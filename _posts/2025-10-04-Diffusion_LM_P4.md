---
layout:     post
title:      "Diffusion Language Models -- Part Four (Post-training with Reinforcement Learning)"
date:       2025-10-04 01:00:00
categories: discrete diffusion
---
{% marginfigure "marginfigure-rlhf-good" "assets/img/rlhf.gif" "Source: PNGs in GIF generated with ChatGPT." %}

{% newthought "Recently, the post-training of large language models (LLMs) with reinforcement learning" %} (RL) has been an important source for the significant progress we are seeing in LLM capabilities (for reasoning, agents/tool-use, planning etc). 

{% marginnote "marginnote-posttrain" "The _post-training_ of an LLM comes after _pretraining_ (which is when LLMs are trained on next-token prediction over web-scale text). It \"polishes\" the LLM into the useful models we are used to interacting with. This _<a href='https://tokens-for-thoughts.notion.site/post-training-101' title='Post-training 101: A hitchhikers guide into LLM post-training'>blog post</a>_ by two Meta Super Intelligence (MSL) researchers gives a good overview of the post-training phase. Much of what is in their blog post would also apply to post-training for diffusion language models (DLMs)." %}

This became especially apparent earlier this year when DeepSeek surprised (and <a href='https://www.cnbc.com/2025/01/27/nvidia-falls-10percent-in-premarket-trading-as-chinas-deepseek-triggers-global-tech-sell-off.html' title='Nvidia drops nearly 17% as China‚Äôs cheaper AI model DeepSeek sparks global tech sell-off'>moved markets</a>) with the release of their R1 model <a href='https://www.nature.com/articles/s41586-025-09422-z' title='DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning'>(Guo et al, 2025)</a>, an auto-regressive LLM (AR-LLM). Their model was post-trained with an efficient RL algorithm (GRPO, see below) in a way that unlocked "thinking" for improved performance on reasoning tasks.{% sidenote "sidenote-reasoning" "The term 'reasoning' with respect to LLMs, also referred to as Large Reasoning Models or LRMs, is still being settled upon. However, there are notable differences between the \"thinking\" traces produced by LRMs and what we might generally accept as reasoning by humans. A good overview of this can be found in <a href='https://arxiv.org/pdf/2504.09762v1' title='(How) Do reasoning models reason?'>(Kambhampati et al, 2025)</a> and this _<a href='https://x.com/rao2z/status/1966969679739768982' title=''>list</a>_." %} Prior to this, however, RL post-training was already crucial for aligning AR-LLM generations towards users' preferred forms/styles of text and conversation, as well as for meeting safety and security requirements. 

In this post, I examine similar RL methods for diffusion language models (DLMs); which will be key for pushing DLMs to parity (or more) with existing AR-LLMs in terms of capabilities. In line with the previous posts of this series, I will focus on Masked-DLMs (which are the keenest focus of current research); on the RL side, my focus will be on online policy-gradient algorithms,{% sidenote "sidenote-policy" "Policy-gradient algorithms are named for how: (i) the <span style='color: red'>policy</span> (the mechanism for generating <span style='color: red'>trajectories</span> i.e. sequences of tokens when used in the context of LLM RL post-training) is in the form of a parameterized model, and (ii) the policy's parameters are learned by following the gradient of some function with respect to those parameters. (_The definitions of the terms in <span style='color: red'>red</span> can be found <a href='#terminology'>below</a>._) They can be categorised as being _online_ or _offline_, where the \"on\"/\"off\" relates to whether the policy is learning from trajectories coming from itself (\"on\") or not (\"off\"; e.g. from another model's distribution). <br><br><ins>_Sidenote:_</ins> offline methods, such as DPO <a href='https://arxiv.org/abs/2305.18290' title='Direct Preference Optimization: Your Language Model is Secretly a Reward Model'>(Rafailov et al, 2023)</a>, were instrumental for aligning LLM to human preferences (e.g. used in the training for Llama 3 <a href='https://arxiv.org/pdf/2407.21783' title='The Llama 3 Herd of Models'>(Llama Team, AI @ Meta, 2024)</a> models). For the interested, similar methods have been proposed for diffusion models: e.g. VRPO used to post-train the original LLaDA Masked-DLM <a href='https://arxiv.org/abs/2502.09992' title='Large Language Diffusion Models'>(Nie et al, 2025)</a> to give LLaDA 1.5 <a href='https://arxiv.org/abs/2505.19223' title='LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models'>(Zhu et al)</a>, as well as ones for continuous diffusion such as Diffusion-DPO <a href='https://arxiv.org/abs/2311.12908' title='Diffusion model alignment using direct preference optimization'>(Wallace et al, 2023)</a> and DSPO <a href='https://openreview.net/forum?id=xyfb9HHvMe' title='Direct Score Preference Optimization for Diffusion Model Alignment'>(Zhu et al, 2025)</a>." %} namely: Group Relative Policy Optimization (__GRPO__) <a href='https://arxiv.org/abs/2402.03300' title='DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models'>(Shao et al, 2024)</a>, <a href='https://arxiv.org/abs/2501.12948' title='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning'>(DeepSeek AI, 2025)</a> and Proximal Policy Optimization (__PPO__) <a href='https://arxiv.org/abs/1707.06347' title='Proximal Policy Optimization Algorithms'>(Schulman et al, 2017)</a>, which (i) are being used in post-training AR-LLM today; and (ii) have been found to reach better performance compared to offline algorithms <a href='' title='Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback'>(Ivison et al, 2024)</a>.

The outline of this post is as follows: I will start by setting the scene with _[1. an accessible introduction to RL post-training using policy-gradient algorithms](#-1-paint-a-picture-of-policy-gradient-rl-in-2-minutes)_, followed by outlining _[2. the main challenge for DLM post-training with such methods](#Ô∏è-2-can-we-just-reuse-the-llm-rl-post-training-methods-that-worked)_. I will then highlight _[3. some proposed approaches for RL post-training of DLMs](#-3-what-policy-gradient-rl-methods-have-been-proposed-for-dlms). If you are familiar with RL post-training for LLMs, you could just skip directly to [section 2](#Ô∏è-2-can-we-reuse-the-rl-post-training-methods-that-worked-for-ar-llms). Otherwise, to fully benefit from this post, going through my earlier posts [Part 1](https://hankelvin.github.io/articles/25/Diffusion_LM_P1), [Part 2](https://hankelvin.github.io/articles/25/Diffusion_LM_P2) and [Part 3](https://hankelvin.github.io/articles/25/Diffusion_LM_P3) for some background to DLMs first would probably be useful._ 

### üñºÔ∏è 1. Paint a picture of RL post-training of LLMs in 5 minutes? 
Before discussing policy-gradient RL for DLMs, let's get to some common ground with an introduction to such methods, as well as a sense of how we are using them with AR-LLMs currently. I always find analogies help us to better grasp complex topics, so let's start with one:

<span style='color: #000080; font-family: Candara'>Imagine you are a parent of a child Jesse, and you want them to learn to give the right answer (let's refer to this as <em>__o__</em>, for output) to this question: _"Levy has two apples in his pocket, Alex has two apples in her bag. They have a picnic and eat one of the apples. How many apples do they have left?"_ (essentially: _"What is 2+2-1 equals to?"_); let's refer to this question as __$$q_{k}$$__. The idea is that it is best to have Jesse give _"3"_ (or similar) as the final answer whenever they encounter __$$q_{k}$$__ (or a similar problem). One way to help Jesse learn this could be to (i) pose __$$q_{k}$$__ to Jesse multiple times, then (ii) have Jesse give an answer each time (let's call each of this __$$o^{k}_{i}$$__), and then (iii) tell Jesse for each __$$o^{k}_{i}$$__ whether it is a good answer.</span> 
<div style="background-color: #249ae9ff; max-width: 50%; color: white; padding: 20px; border-radius: 8px; margin: 10px;">
  <h3 style="margin: 0 0 15px 0; width: 100%;font-family: Candara">Some possible answers of Jesse's</h3>   
  <p style="margin: 0; width: 100%; text-align: left; font-size: 16px; font-family: Candara">   
  üí¨ <em>o</em><sub>1</sub>: I know this, the answer is 3! 
  <br>
  üí¨ <em>o</em><sub>2</sub>: I don't know, the answer is 3? 
  <br>
  üí¨ <em>o</em><sub>3</sub>: I love chicken nuggets! I will never eat apples!
  <br>
  üí¨ <em>o</em><sub>4</sub>: Levy has 2 apples and they eat 1 so there has to be 3 apples left. 
  <br>  
  üí¨ <em>o</em><sub>5</sub>: Three!
  <br>  
  üí¨ <em>o</em><sub>6</sub>: They have 2 plus 2 apples so that is 4 apples. They eat one, so 4 minus 1, that means they have 3 apples left. Duh!
  </p>
</div>

We can also see from above that some of the answers that Jesse might come up with could be better than others; in terms of correctness (in the final answer, and in the reasoning) as well as for style.{% sidenote "sidenote-Jesse" "Some answers are clearly off (i.e. <em>o</em><sub>3</sub>). Some (i.e. <em>o</em><sub>4</sub>) give the correct final answer, but have a wrong reasoning process for getting to it. Some might be nearly identical, but one amonsgt them is preferred over another, e.g. <em>o</em><sub>1</sub> versus <em>o</em><sub>2</sub> (for a more confident Jesse). Others might be quite different yet slightly preferred over another, e.g. <em>o</em><sub>5</sub> versus <em>o</em><sub>6</sub> (depending on whether we prefer to have a Jesse that will give a reasoning along with their answer, but in a sassy way)." %} Therefore, we can expect to have some preferences between each __$$o^{k}_{i}$$__, and hence we might want to steer Jesse's mind such that whenever Jesse encounters __$$q_{k}$$__, ideally Jesse gives __$$o^{k}_{i}$$__ that is most preferable. 

<ins>In essence, what we want to do for Jesse is similar to what we want to do with LLMs using policy-gradient RL post-training!</ins> i.e. we want an LLM to learn, by updating its parameters, that when presented with a certain prompt __$$q_{k}$$__ (or similar) it should generate responses that are preferred (achieve highest reward). This is done via getting the LLM to give higher likelihoods for the sequence of tokens in the higher-scoring __$$o^{k}_{i}$$__. 

<div style="background-color: #AFEEEE; max-width: 50%; color: black; padding: 20px; border-radius: 8px; margin: 10px;" id='terminology'>
  <h3 style="margin: 0 0 15px 0; width: 100%;">Some terminology</h3>   
  <p style="margin: 0; width: 100%; text-align: left; font-size: 16px;">
  Before proceeding, let's set the definitions of some key RL terms first. Each of these terms are also associated with concepts (in brackets and <span style='color: #000080'>blue</span> below) from the Jesse example, so as to connect them with RL on AR-LLMs.</p>
  <br>
  <p style="margin: 0; width: 100%; text-align: left; font-size: 16px;">   
  ‚ñ™Ô∏è <span style='color: red'>"state"</span>: information about the current situation at a given moment in time; 
  <br>
  ‚ñ™Ô∏è <span style='color: red'>"action"</span>: a decision/choice that can be taken at the point of a certain state; 
  <br>
  ‚ñ™Ô∏è <span style='color: red'>"trajectory"</span>: a sequence of states and actions that can be taken (<span style='color: #000080'><em>o<sup>k</sup><sub>i</sub></em></span>); 
  <br>
  ‚ñ™Ô∏è <span style='color: red'>"policy"</span>: some model that can give us trajectories (<span style='color: #000080'>Jesse</span>); 
  <br>
  ‚ñ™Ô∏è <span style='color: red'>"reward"</span>: feedback on a trajectory, i.e. what can be gotten if the trajectory is taken (<span style='color: #000080'>whether <em>o<sup>k</sup><sub>i</sub></em> is good or bad/how good or how bad</span>); 
  <br>
  ‚ñ™Ô∏è <span style='color: red'>"reward model"</span>: some method/model giving the reward for a trajectory (<span style='color: #000080'>you!</span>). 
  <br>
  ‚ñ™Ô∏è <span style='color: red'>"advantage"</span>: how much better taking action <em>a<sub>t</sub></em> at state <em>s<sub>t</sub></em> is compared to the average of all actions possible. 
  <br>
</p>
</div>

{% marginnote "marginnote-example" "To make the definitions more concrete let's shift the example with Jesse above to an AR-LLM: Let's say we are at the point in time (_state_) where the AR-LLM has just processed the prompt $$q_{k}$$ fed to it. Let's call this state $$s_{0}$$. For the sake of this example, let us assume that the AR-LLM can only ever give answers to $$q_{k}$$ from the 6 examples above (i.e. <em>o</em><sub>1</sub> to <em>o</em><sub>6</sub>). If we prefer <em>o</em><sub>6</sub> the most, then the _action_ we want from the AR-LLM immediately after $$s_{0}$$ is to return the word \"They\" _(in the next-token prediction set-up of AR-LLMs, this means striving to give this word the highest probability)_. The objective is to have the AR-LLM learn to return a sequence (i.e. _trajectory_) of state-action decisions so as to give an answer that obtains as high a reward as possible. Note that the learning for the policy also involves cases such as these: if the action chosen was to return \"I\" after $$s_{0}$$, then the AR-LLM should learn that at such $$s_{1}$$, the word \"know\" should have the highest probability (applies if we prefer <em>o</em><sub>1</sub> over all the other answers (<em>o</em><sub>2</sub> and <em>o</em><sub>3</sub>) that start with \"I\"). and so on and so forth..." %}

In practice, we achieve this by getting the LLM (the policy) to generate a diverse set of answers for a given prompt $$q_{k}$$ by using a sufficiently high sampling <a href='https://huggingface.co/blog/how-to-generate#:~:text=the%20so%2Dcalled-,temperature,-of%20the%20softmax' title='How to generate text: using different decoding methods for language generation with Transformers'>temperature</a>. The LLM learns via the feedback from the rewards of different experiences (i.e. pairs of __$$q_{k}, o^{k}_{i}$$__) which is the best answer to give.

<p style='size: 22pt; font-weight: bold;'>PPO and GRPO briefly: efficient & stable training</p>
(_Bear with me, just a little more common ground... üòÖ, so that we can situate the next section properly._) In this section, I zoom in to focus on two aspects shared by the PPO and GRPO algorithms; a sense of these aspects are necessary for me to be able to explain the key points of the subsequent sections.{% sidenote "sidenote-fuller" "I give a very general view here, but there is a fair bit more behind both algorithms; for a fuller understanding of them take a look at the following resources to start: <a href='https://yugeten.github.io/posts/2025/01/ppogrpo/' title='A vision researcher‚Äôs guide to some RL stuff: PPO & GRPO'>this post by Jimmy Shi</a>, <a href='https://rlhfbook.com' title='Reinforcement Learning from Human Feedback'>this series by Nathan Lambert</a> and <a href='https://huggingface.co/blog/deep-rl-ppo' title='Proximal Policy Optimization (PPO)'>this HuggingFace RL course unit</a>." %}

‚ñ™Ô∏è A major preoccupation for RL training in general (i.e. including PPO/GRPO) is to find some balance between __exploration__ (i.e. generating diverse answers to receive useful feedback for learning) and __exploitation__ (i.e. leveraging useful knowledge the policy has learned from past encounters, e.g. from Jesse's <em>o</em><sub>5</sub> which gets a good reward).{% sidenote "sidenote-stability" "The trade-off is as follows: ‚ñ™Ô∏è allowing more exploration (i.e. via generating the __$$o^{k}_{i}$$__ trajectories by sampling with high temperature) results in very sparse signals (to go the extreme: imagine that for every __$$q_{k}$$__, we have to generate all the possible combinations of words in English almost all of which would have very low reward with respect to __$$q_{k}$$__) and wastes compute; whereas, on the other hand, ‚ñ™Ô∏è relying on already learned knowledge (e.g. generating __$$o^{k}_{i}$$__ by sampling with low temperature) may keep the policy around poor/sub-optimal outputs i.e. does not allow it to reach an optimal __$$o^{k}_{i}$$__." %} When applying PPO/GRPO to AR-LLMs, the bottleneck is the generating of trajectories (due to the generation process being auto-regressive) and it typically takes up most of the training run-time. Hence, it is typical to reuse the same set of sampled trajectories for a few more update steps{% sidenote "sidenote-mu" "This is _\"K epochs\"_ in Algorithm 1 of the PPO paper <a href='https://arxiv.org/abs/1707.06347' title='Proximal Policy Optimization Algorithms'>(Schulman et al, 2017)</a> and `num_ppo_epochs` in the [TRL implementation](https://huggingface.co/docs/trl/main/en/ppo_trainer#trl.PPOConfig); the $$\mu$$ hyperparameter in Algorithm 1 of the DeepSeek Math (GRPO) paper <a href='https://arxiv.org/abs/2402.03300' title='DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models'>(Shao et al, 2024)</a> and `num_iterations` in the [TRL implementation](https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOConfig)." %} to squeeze more learning out of them. _Hereon, I will use the term __$$\mu$$-updates__ to refer to these update steps._ Think of it in this way: although going through one round of ($$q_{k}, o^k_1... o^k_6$$) with Jesse might help them get a little closer to giving the most preferred output, but it might not be sufficient... so we repeat with multiple rounds of ($${k}, o^k_1... o^k_6$$) to help Jesse learn.{% sidenote "sidenote-onoff" "<ins>_Sidenote:_</ins> While PPO and GRPO are recognised as online methods, a case could be made that these subsequent $$\mu$$-updates after the first step/epoch, are at least _slightly off-policy_ <a href='https://arxiv.org/abs/2505.17508' title='On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning'>(Zhang et al, 2025)</a>... especially when $$\mu$$ is set to a large number." %}
 
‚ñ™Ô∏è Another major preoccupation (for policy-gradient methods in general) is achieving __stable training__ to facilitate successful policy learning.{% sidenote "sidenote-variance" "Since we typically train across diverse problems $$q_k$$ that each have their own reward distributions, this adds to the variance in the gradient estimates (which is already present between trajectories of a given $$q_k$$); therefore, when taking update steps, large updates can overfit the policy to some problems at the expense of others, leading to instability and hindering overall learning." %} Hence, one of the design principles in PPO <a href='https://arxiv.org/abs/1707.06347' title='Proximal Policy Optimization Algorithms'>(Schulman et al, 2017)</a> was to ensure stability across update steps. This was done by adding the following to the training objective of vanilla policy-gradient methods (e.g. REINFORCE <a href='https://dl.acm.org/doi/10.1007/BF00992696' title='Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning'>(Williams, 1992)</a>): (i) a __KL-regularisation__ term;{% sidenote "sidenote-kl" "The _[KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)_ is a measure of how close/apart one distribution ($$P$$) is to another ($$Q$$); it is an asymmetric measure; so KL of $$P || Q$$ is not the same as KL of (Q||P)." %} and (ii) the use of clipping as a floor/ceiling on the update. These help avoid updates to the policy that veer too far from some \"trusted\" zone of some reference policy that has already been established (for e.g. from explorations in previous updates, or an initial SFT-ed policy). Since GRPO is actually based upon PPO,{% sidenote "sidenote-grpomods" "Doing away with the need for a separate memory- and compute-heavy value model to assess advantage, replacing it with a group-based advantage estimation." %} a similar objective to PPO can also be found there.{% marginfigure "marginfigure-ppogrpo" "assets/img/grpo.png" "Image: PPO and GRPO; their similarities and differences -- source: <a href='https://arxiv.org/abs/2402.03300' title='DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models'>(Shao et al, 2024)</a>. Note that there are variants of PPO that permit generating multiple trajectories, computing their rewards and advantages in one pass (similar to the GRPO figure) but still needing a value model. " %} 

_It is these -- the __KL regularisation term__ and the __$$\mu$$-updates__ -- that presents some challenges to overcome (as well as opportunities to leverage as in __diffuGRPO__) for the use of PPO/GRPO on DLMs, and we will discuss these next... (Note: The rest of this post will go into the weeds on these points and will be more technical.)_ 

### ‚ôªÔ∏è 2. Can we reuse the PPO/GRPO methods that worked for AR-LLMs?
The short answer is... <span style='color: blue'>broadly, yes</span> but with the need for some <span style='color: blue'>non-trivial modifications</span> to address the issue of how to obtain the likelihood for trajectories from a Masked-DLM. These likelihoods are needed in two places in the PPO/GRPO objective: (i) for an [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling) weight, as well as (ii) an estimate for the KL-regularisation term.{% sidenote "sidenote-grpo-obj" "We use the GRPO objective to illustrate (_with clipping omitted to reduce clutter in the equation_): 
$$
\begin{aligned}
L_{GRPO}(\theta) = - \frac{1}{\sum_{i=1}^{G} |o_i|} \sum_{i=1}^{G} \sum_{t=1}^{|o_i|} 
\\\\
\bigg[ {\color{red}\frac{\pi_\theta(o_{i,t}|x, o_{i,<t})}{{\left[ \pi_\theta(o_{i,t}|x, o_{i,<t}) \right]}_{\text{no grad}}}} \hat{A}_{i,t}  {\color{blue} - \beta D_{\text{KL}}[\pi_\theta \| \pi_{\text{ref}}} ] \bigg]
\end{aligned}
$$
<br>
where $$\pi_{\theta}$$ is the current policy and $$\pi_{ref}$$ is either the initial (typically obtained via SFT) or some earlier-update $$\pi_{\theta}$$. 
<br>
As we can see:
<br>
‚ñ™Ô∏è the per-token likelihoods (obtained twice, once with gradients through the policy $$\pi_{\theta}$$ and another without gradients) are used in the first term (in <span style='color: red'>red</span>). The ratio of these corresponds to an [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling) on the advantages $$\hat{A}_{i,t}$$ (to address that trajectories are coming slightly off-policy in the __Œº-updates__ steps);
<br>
‚ñ™Ô∏è the _KL-regularisation term_ is in <span style='color: blue'>blue</span>; and this is where the sequence-level likelihoods are used. In practice, this KL estimate is implemented via this form: `KL` $$ = e^r - r - 1$$ where $$r = log( \pi_{\theta}(o_{i,t}|x, o_{i,<t}) / \pi_{ref}(o_{i,t}|x, o_{i,<t}) )$$. The per-token likelihood for $$\pi_{\theta}$$ above can be reused, and only the ones from $$\pi_{ref}$$ need to be computed here. For a concrete feel: see the implementation in [TRL](https://github.com/huggingface/trl/blob/e086f073cf6dee30acc2d3fe357db21e1901c2be/trl/trainer/grpo_trainer.py#L1719). 
<br><br><ins>_Sidenote:_</ins> Recent studies <a href='https://arxiv.org/abs/2505.17508' title='On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning'>(Zhang et al, 2025)</a> and <a href='' title='On a few pitfalls in KL divergence gradient estimation for RL'>(Tang et al, 2025)</a> have established that there are non-trivial differences relating to a set of fine-grained choices of the method and implementation for the KL divergence estimate. <span style='color: blue'>__Note that these have implications for online RL of DLMs due to the need to estimate these estimates there (see _<a href='#dlm_estimate'>below</a>_). To my mind, these two pieces are recommended reading for RL on DLMs.__</span> 
<br><br><ins>_Sidenote:_</ins> If the beta (Œ≤) coefficient, which controls the amount of KL-regularisation in PPO/GRPO, is set to be zero, then there is no need for the sequence-level likelihoods. Empirically, there have been reports recently that the KL-regularisation may not be necessary for AR-LLMs (quite likely under certain training setups i.e. hyperparameter setting, modeling choice where the encountered KL divergences between $$\pi_{\theta}$$ and $$\pi_{ref}$$ are low). See for e.g. <a href='https://ai.meta.com/research/publications/cwm-an-open-weights-llm-for-research-on-code-generation-with-world-models/' title='CWM: An Open-Weights LLM for Research on Code Generation with World Models'>(Copet et al, 2025)</a>; page 13 of paper." %}  

Computing these likelihood for trajectories is easy for AR-LLMs because of how they factorise sequence probabilities at a token-level; i.e. at each step, the AR-LLM predicts from its vocabulary the most likely token to generate. As a result, it is very easy to compute what an AR-LLM thinks is the likelihood of any sequence of tokens (by chain-rule, i.e. simply summing the log-probabilities for each token of the sequence).{% sidenote "sidenote-factorise" "See also footnote 27 in the _<a href='https://hankelvin.github.io/articles/25/Diffusion_LM_P2#:~:text=lower%20bound%20(ELBO)-,Unlike,-AR%2DLLMs%20which' title='Diffusion Language Models -- Part Two (What kinds are there and how is one trained?)'>second post</a>_ of this series." %} 
 
However, this is <span id='dlm_estimate'>not the case üòµ‚Äçüí´</span> for Masked-DLMs (and discrete diffusion models generally). Although we do get probabilities for tokens at each step of the diffusion generation process (which is what allows us to decide which token to unmask into), each of these steps is a denoising one <ins>that depends on all its preceding steps</ins>. In other words, computing sequence probabilities for DLMs require going through multiple denoising steps (from $$T$$ to 0). To have to keep doing this for every sampled trajectory during online RL training with PPO/GRPO is very computationally expensive, and will be significantly worse for very long sequences.{% sidenote "sidenote-efficient" "Although the efficient DLM methods I covered in the previous post (such as Block Diffusion <a href='https://openreview.net/forum?id=tyEyYT267x' title='Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models'>(Arriola et al, 2025)</a>) can help alleviate this, the increase in computation required -- compared to what is required in AR-LLMs -- will still be substantial." %} As such, there is a need to establish ways to efficiently, yet as accurately as possible, estimate these likelihoods with the DLM. This is the focus of much research currently and we will look into in the next section.
 
### üí° 3. What have been proposed for DLMs?
This section outlines two research trends in online RL algorithms to Masked-DLMs. All of these started with __diffuGRPO__ <a href='https://arxiv.org/abs/2504.12216?' title='d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning'>(Zhao et al, 2025)</a>, which landed in Q1 this year and was the first work to explore a way of bringing online RL algorithms to Masked-DLMs. __diffuGRPO__ and the initial wave of research is distinguished by their main contributions for ways to estimate likelihoods with Masked-DLMs, another more recent wave (released in the last month or so) begin to explore extensions for Masked-DLMs with semi-autoregressive generation for longer generations and with more efficiency (e.g. with KV caching). 

<p style='size: 22pt; font-weight: bold'>Efficient likelihood estimation for online RL on Masked-DLMs</p>
Each of the three pieces of work mentioned here proposed a way to do the likelihood estimation. Note that although they were formulated for GRPO, it should be possible to leverage their likelihood approaches for use in a PPO setup. 

‚óºÔ∏è <span id='diffugrpo'>__diffu-GRPO__</span> <a href='https://arxiv.org/abs/2504.12216?' title='d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning'>(Zhao et al, 2025)</a>: estimate the per-token likelihood of a trajectory by simply doing unmasking in one-step.{% sidenote "sidenote-diffugrpo" "In practical terms, this is done as follows: for a given prompt $$q_{k}$$ append it with a fully-masked continuation (i.e. max sequence generation length) and pass it through the Masked-DLM; this output is the estimated per-token probability distribution (conditioned by the prompt $$q_{k}$$)." %} As noted above, such one-/few-step unmasking does not reflect the multi-step denoising in Masked-DLM -- hence and quite importantly, their proposal hinges on (i) the $$\mu$$-updates typically (but not mandatorily) used in GRPO, and (ii)  a random masking to the _prompt $$q_{k}$$_ portion of the input (i.e. input = $$q_{k}$$ + fully masked continuation). At every of the $$\mu$$-steps, the mask is randomised but always fixed at 15%.{% sidenote "sidenote-d1masking" "See Appendix A of paper: _\"In gradient update iterations, each token in the prompt is randomly masked with a probability pmask = 0.15 for log-probability estimation.\"_ " %} We can see this as obtaining slightly varied likelihood estimates for a set of inputs closely resembling the prompt $$q_{k}$$ which according to the authors _"acts a form of regularization for policy optimization"_. As for estimating sequence-level likelihood of a trajectory: the authors assume [mean-field decomposition](https://en.wikipedia.org/wiki/Mean-field_theory) (i.e. a series of localized independent distributions can be useful for approximating a complex conditional distribution), allowing them to simply sum the trajectory's per-token log probabilities to get this estimate. At least two pieces of empirical support are available for __diffu-GRPO__: (i)  <a href='https://arxiv.org/abs/2504.12216?' title='d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning'>(Zhao et al, 2025)</a> reported consistently stronger performance on four different math and puzzle/planning logical problems;{% sidenote "sidenote-diffugrpo-results" "See Table 1 and Figure 5 of their <a href='https://arxiv.org/abs/2504.12216?' title='d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning'>paper</a>" %} and (ii) the same approach for obtaining per-token and sequence likelihood was adopted by __IGPO__ <a href='https://arxiv.org/pdf/2509.10396' title='Inpainting-Guided Policy Optimization for DiffusionLarge Language Models'>(Zhao et al, 2025b)</a> as well and tested successfully on reasoning benchmarks there.
<br><br>
{% maincolumn "assets/img/diffugrpo.png" "Image: likelihood estimation approach in __diffuGRPO__ via one-step denoising (varied mask on prompt tokens across $$\mu$$-updates) -- source: <a href='https://arxiv.org/abs/2504.12216?' title='d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning'>(Zhao et al, 2025)</a>.<br><br>" %}

‚óºÔ∏è __coupled-GRPO__ <a href='https://arxiv.org/abs/2506.20639' title='DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation'>(Gong et al, 2025)</a>: departs from __diffu-GRPO__ in that they apply the masking to the continuation portion (i.e. the $$o^k_i$$) for _each trajectory_. They also use two samples on _each trajectory_ at every $$\mu$$-step for the estimation (compared to __diffu-GRPO__'s use of only one sample of _each prompt $$q_{k}$$_ at every $$\mu$$-step, which requires much less computation). Each of the two samples masks different, but paired (henced the "coupled" in the name) parts of the continuation.{% sidenote "sidenote-coupled" "This involves sampling a random timestep $$t$$, and then setting the other $$\hat{t}$$ so that (i) $$t + \hat{t} = T$$, the terminal timestep (i.e. 1.0). Then, what is masked for $$t$$ is not masked in $$\hat{t}$$ and vice-versa." %} This ensures that (i) every token in a trajectory is involved once in the estimation giving _"each token a non-zero learning signal"_, and (ii) it also more closely mimics the denoising generation process in Masked-DLMs (where probabilities are produced on partially masked continuations). __coupled-GRPO__ was used by <a href='https://arxiv.org/abs/2506.20639' title='DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation'>(Gong et al, 2025)</a> in their training for DiffuCoder, a code generation-focused Masked-DLM, who stated that __coupled-GRPO__ was formulated in response to their findings that __diffu-GRPO__'s likelihood approximation methods do not _"yield a stable reward improvement..., probably because code tasks demand higher token-level generation accuracy than math tasks"_. They go on to show that __coupled-GRPO__ leads to stronger performance through stabler rewards (see left and centre chart of Figure 7 in their paper) over __diffu-GRPO__ for coding tasks, as well as for another baseline where they remove the masking coupling of ($$t, \hat{t}$$). Interestingly they also found that coupled-GRPO required sampling trajectories at a higher temperature for success (see right chart of Figure 7 in their paper) which has congruence with similar findings on online RL for AR-LLMs recently <a href='https://arxiv.org/abs/2505.24864' title='ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models'>(Liu et al, 2025)</a>. 
<br><br>
{% maincolumn "assets/img/coupled-grpo.png" "Image: likelihood estimation approach in __Coupled-GRPO__ to balance coverage and reduce variance -- source: <a href='https://arxiv.org/abs/2506.20639' title='DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation'>(Gong et al, 2025)</a>." %}

‚óºÔ∏è __uni-GRPO__ <a href='https://arxiv.org/abs/2505.15809' title='MMaDA: Multimodal Large Diffusion Language Models'>(Yang et al, 2025)</a>: was applied in the training procedure for MMaDA, a Masked-DLM with multi-modal (vision and text) capabilities. It is similar to __coupled-GRPO__ in that it also masks on the continuation to obtain the likelihood estimates. Specifically, the noise for each $$\mu$$-step update is randomly sampled from a uniform distribution (instead of the fixed 15% of the prompt in __diffu-GRPO__, which also meant that the same timestep (i.e. $$T$$) across all samples was used there). Only one sample on _each trajectory_ at every $$\mu$$-step is taken (i.e. more computation than __diffu-GRPO__ but less than __coupled-GRPO__). In a departure from the other two approaches, the per-token likelihood is computed with the masked tokens (i.e. this relates to the ELBO of the Masked-DLM),{% sidenote "sidenote-elbo" "See Equation 3 of  the MMaDA paper for __uni-GRPO__; and compare with Equation 4 of the DiffuCoder paper for __coupled-GRPO__."%} and the sequence level likelihood _"is then approximated by averaging over masked tokens"_ (see Equation 4 in paper). Taking the ELBO as the estimate is quite a meaningful departure from the __diffu-GRPO__ and __coupled-GRPO__ approaches, and although it has a theoretical connection to the Masked-DLM training objective, it is not clear that it provides a better estimate for online RL training compared to the case where all tokens are considered (as in __coupled-GRPO__); nonetheless, it is clear that __uni-GRPO__ outperforms __diffu-GRPO__ (likely due to the larger-sized sampling, i.e. every trajectory every $$\mu$$-step).{% sidenote "sidenote-compare" "See comparisons of their performance in Figure 3 in ¬ß5.2 of the MMaDA paper and Table 1 of the IGPO paper that also leverages __diffu-GRPO__." %}

<p style='size: 22pt; font-weight: bold'>Other proposals for Masked-DLMs</p>

‚óºÔ∏è __wd1__ <a href='https://arxiv.org/abs/2507.08838' title='wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models'>(Tang et al, 2025)</a>: proposes a few modifications to the GRPO objective which allow it to be used on a Masked-DLM with likelihood evaluation through one policy only (the current policy $$\pi_{\theta}$$). This is desirable as it is much more computationally efficient compared to the approaches above, which needed to do so for the policy before the $$\mu$$-update ($$\pi_{old}$$) and the reference policy ($$\pi_{ref}$$). Briefly, their approach hinges on shifting from (i) applying the importance sampling to the advantage (as per original PPO; see above) to (ii) applying a reverse KL-divergence penalty.{% sidenote "sidenote-wd1" "See ¬ß3.1 and Equation 3 of their <a href='https://arxiv.org/abs/2507.08838' title='wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models'>paper</a>."%} This enables their derivation of an expression on the GRPO objective that only needs likelihood estimates from $$\pi_{\theta}$$.{% sidenote "sidenote-wd1-obj" "In order to obtain their expression of the GRPO objective, it also involves shifting the trajectory sampling (from $$\pi_{old}$$) to a geometric mixture of $$\pi_{old}$$ and $$\pi_{ref}$$ (see ¬ß3.1 of their paper)." %} They report obtaining up to 16% better performance over __diffuGRPO__ on math and logic/puzzle planning benchmarks even without having to do an SFT phase (which is, on the other hand, needed for __diffuGRPO__ to reach reasonable performance).{% sidenote "sidenote-wd1-results" "<ins>_Sidenote:_</ins> Although, a case could be made that the settings <a href='https://arxiv.org/abs/2507.08838' title='wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models'>(Tang et al, 2025)</a> use for comparison with __diffuGRPO__ might not be fully like-for-like. Their wd1 objective is obtained assuming the $$\beta$$-controlled KL-regularisation term (see above) is included in the GRPO objective (see their Equation 5). While the final expression of the wd1 objective does away with the KL-regularisation term and does not include it explicitly, the controlling $$\beta$$ term remains embedded throughout the wd1 objective (their Equations 9 and 6). Yet in practice $$\beta$$ is set to 0 (see \"Implementation\" in ¬ß4 of their paper) for the model trained with wd1 in their experiments, in effect this leaves out the consideration of any KL-regularisation. On the other hand, the $$\beta$$ from the original __diffuGRPO__ paper (0.04) was kept (see Table 5 of Appendix B.4). Perhaps it will be helpful to also understand how __diffuGRPO__ performs without the KL-regularisation applied." %} Notably however, it does not appear that this approach leads to stronger empirical outcomes when compared with __uniGRPO__.{% sidenote "sidenote-wd1-less" "Compare the reported scores on GSM8K and MATH500 by <a href='https://arxiv.org/pdf/2509.10396' title='Inpainting-Guided Policy Optimization for DiffusionLarge Language Models'>(Zhao et al, 2025b)</a> (refer to Table 1) and <a href='https://arxiv.org/abs/2507.08838' title='wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models'>(Tang et al, 2025)</a> (refer to Table 3; look at the 256-length results as the other paper used a 256 length setting -- see Appendix A there). "%}

<!-- Note however, a slight discrepancy: the __diffuGRPO__ results reported by <a href='https://arxiv.org/pdf/2509.10396' title='Inpainting-Guided Policy Optimization for DiffusionLarge Language Models'>(Zhao et al, 2025b)</a> is actually higher than the figures report by <a href='https://arxiv.org/abs/2507.08838' title='wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models'>(Tang et al, 2025)</a>. -->

‚óºÔ∏è __IGPO__ <a href='https://arxiv.org/pdf/2509.10396' title='Inpainting-Guided Policy Optimization for DiffusionLarge Language Models'>(Zhao et al, 2025b)</a>: shares the same first author as __diffu-GRPO__, and as mentioned above, uses the same likelihood estimation approach as __diffu-GRPO__. The novelty here is a procedure to leverage the inpainting capabilities inherent in DLMs (see my first [post](https://hankelvin.github.io/articles/25/Diffusion_LM_P1#:~:text=strategies%20such%20as-,infilling,-.%20In%20the%20GIF)) to optimise the training efficiency and efficacy of GRPO. In GRPO,{% sidenote "sidenote-refer" "Refer to the image of PPO vs GRPO in the margins above for a sense." %} when the entire group of sampled trajectories for a given prompt $$q_{k}$$ (for e.g. a math problem) is zero, this results in no useful signal for the model to update its parameters. __IGPO__'s proposal assumes access to ground-truth or sufficiently high quality reasoning traces for $$q_{k}$$ and to use some segment of the reasoning traces when such zero-reward groups are encountered. Specifically, by "seeding" a fragment of the reasoning trace amongst the masked tokens, we get a chance to steer the Masked-DLM towards generating a trajectory of good quality,{% sidenote "sidenote-hint" "It is akin to hinting to Jesse _\"They have 2 plus 2 apples so that is 4 apples...\"_" %} which is then swopped with a zero-reward trajectory from the group. Training with this way to avoid zero-reward update steps led to stabler learning and enabled improvements on math and planning benchmarks over __diffuGRPO__; but importantly, it also outperforms __uniGRPO__ that requires more samples to be taken for the likelihood estimation (per-trajectory per $$\mu$$-update vs per-prompt per $$\mu$$-update).{% sidenote "sidenote-igporesults" "See Table 1 of their paper <a href='https://arxiv.org/pdf/2509.10396' title='Inpainting-Guided Policy Optimization for DiffusionLarge Language Models'>(Zhao et al, 2025b)</a>."%} 

‚óºÔ∏è __TraceRL__ <a href='https://arxiv.org/abs/2509.06949' title='Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models'>(Wang et al, 2025)</a>: encapsulates some of the latest developments on a few fronts in Masked-DLM research. To summarise, they propose the use of a value model (another DLM) to manage the variance across updates (_√† la_ PPO). In addition, they leverage the semi-autoregressive approach of Fast-dLLM <a href='https://arxiv.org/abs/2505.22618' title='Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding'>(Wu et al, 2025)</a>, which I discussed in my previous [post](https://hankelvin.github.io/articles/25/Diffusion_LM_P3#:~:text=There%20are%20a-,number,-of%20approaches%20proposed), that denoises blocks of tokens auto-regressively with efficiency via the use of approximated KV caches. This has the effect of giving a speed up to trajectory sampling, easing a major bottleneck especially for problems that are best solved with lengthy reasoning traces. To put these extensions together required special treatment (e.g. their ¬ß4.3), and this work is notable for putting forward a proposed solution for doing so. They report impressive performance on math benchmarks (87.4 for GSM8K and 94.2 for MATH500) that outperform the other methods listed above in this section,{% sidenote "sidenote-tracerl" "See Table 2 of their paper <a href='https://arxiv.org/abs/2509.06949' title='Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models'>(Wang et al, 2025)</a>; and compare against results report in the other papers." %} as well as ones for coding. Helpfully, the authors released the TraDo series of [4B/8B parameters Masked-DLMs](https://huggingface.co/collections/Gen-Verse/trado-series-68beb6cd6a26c27cde9fe3af) that they trained with this approach, alongside their codebase.

### ü§î 4. What's next?
To sum up, in this post, I started with a general overview of online RL post-training for LLMs. With some common ground established on that, I highlighted the main challenge for extending existing methods for AR-LLMs to Masked-DLMs, which is the issue of how to efficiently and accurately estimate token and sequence likelihoods needed for the PPO/GRPO objective. Finally, I gave an outline for recent work bringing PPO/GRPO to Masked-DLMs, focusing on how they proposed to address this estimation challenge. 

This wraps up the first round on the topics I intended to cover in this series; the next posts -- probably slightly further out -- would look at all the areas I have discussed so far, but with multi-modality in consideration. 